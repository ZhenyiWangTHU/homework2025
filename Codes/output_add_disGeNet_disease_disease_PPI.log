Reading graph:   0%|          | 0/1583254 [00:00<?, ?it/s]Reading graph:  14%|█▍        | 221489/1583254 [00:00<00:00, 2214453.90it/s]Reading graph:  29%|██▉       | 465208/1583254 [00:00<00:00, 2345324.40it/s]Reading graph:  44%|████▍     | 699741/1583254 [00:00<00:00, 2324061.29it/s]Reading graph:  59%|█████▉    | 932170/1583254 [00:00<00:00, 2322893.04it/s]Reading graph:  74%|███████▎  | 1164473/1583254 [00:00<00:00, 2320103.70it/s]Reading graph:  88%|████████▊ | 1396492/1583254 [00:00<00:00, 2144338.05it/s]                                                                             Reading graph:   0%|          | 0/873987 [00:00<?, ?it/s]Reading graph:  25%|██▌       | 222284/873987 [00:00<00:00, 2222630.15it/s]Reading graph:  51%|█████     | 444548/873987 [00:00<00:00, 2208646.13it/s]Reading graph:  76%|███████▌  | 665419/873987 [00:00<00:00, 2189986.97it/s]                                                                           Total nodes: 8277916
Epoch: 001, Train Loss: 0.6491, Validation Loss: 0.6704
Total nodes: 8273102
Epoch: 002, Train Loss: 0.6255, Validation Loss: 0.6301
Total nodes: 8275504
Epoch: 003, Train Loss: 0.6215, Validation Loss: 0.6171
Total nodes: 8276268
Epoch: 004, Train Loss: 0.6200, Validation Loss: 0.6107
Total nodes: 8277482
Epoch: 005, Train Loss: 0.6191, Validation Loss: 0.6059
Total nodes: 8275414
Epoch: 006, Train Loss: 0.6184, Validation Loss: 0.6030
Total nodes: 8272679
Epoch: 007, Train Loss: 0.6178, Validation Loss: 0.6013
Total nodes: 8271519
Epoch: 008, Train Loss: 0.6174, Validation Loss: 0.6008
Total nodes: 8276108
Epoch: 009, Train Loss: 0.6171, Validation Loss: 0.6003
Total nodes: 8270317
Epoch: 010, Train Loss: 0.6168, Validation Loss: 0.5991
Total nodes: 8271126
Epoch: 011, Train Loss: 0.6165, Validation Loss: 0.5990
Total nodes: 8279635
Epoch: 012, Train Loss: 0.6163, Validation Loss: 0.5997
Total nodes: 8278715
Epoch: 013, Train Loss: 0.6162, Validation Loss: 0.5973
Total nodes: 8274934
Epoch: 014, Train Loss: 0.6160, Validation Loss: 0.5971
Total nodes: 8274575
Epoch: 015, Train Loss: 0.6160, Validation Loss: 0.5961
Total nodes: 8271845
Epoch: 016, Train Loss: 0.6158, Validation Loss: 0.5985
Total nodes: 8275687
Epoch: 017, Train Loss: 0.6157, Validation Loss: 0.5986
Total nodes: 8276499
Epoch: 018, Train Loss: 0.6157, Validation Loss: 0.5977
Total nodes: 8272084
Epoch: 019, Train Loss: 0.6155, Validation Loss: 0.5958
Total nodes: 8272473
Epoch: 020, Train Loss: 0.6155, Validation Loss: 0.5974
Total nodes: 8278584
Epoch: 021, Train Loss: 0.6154, Validation Loss: 0.5949
Total nodes: 8278001
Epoch: 022, Train Loss: 0.6153, Validation Loss: 0.5948
Total nodes: 8275016
Epoch: 023, Train Loss: 0.6153, Validation Loss: 0.5958
Total nodes: 8278625
Epoch: 024, Train Loss: 0.6153, Validation Loss: 0.5947
Total nodes: 8275682
Epoch: 025, Train Loss: 0.6152, Validation Loss: 0.5955
Total nodes: 8273774
Epoch: 026, Train Loss: 0.6152, Validation Loss: 0.5940
Total nodes: 8277688
Epoch: 027, Train Loss: 0.6151, Validation Loss: 0.5968
Total nodes: 8269757
Epoch: 028, Train Loss: 0.6150, Validation Loss: 0.5935
Total nodes: 8271072
Epoch: 029, Train Loss: 0.6150, Validation Loss: 0.5935
Total nodes: 8274663
Epoch: 030, Train Loss: 0.6150, Validation Loss: 0.5929
Total nodes: 8272868
Epoch: 031, Train Loss: 0.6150, Validation Loss: 0.5950
Total nodes: 8272324
Epoch: 032, Train Loss: 0.6149, Validation Loss: 0.5938
Total nodes: 8273963
Epoch: 033, Train Loss: 0.6149, Validation Loss: 0.5933
Total nodes: 8276030
Epoch: 034, Train Loss: 0.6149, Validation Loss: 0.5932
Total nodes: 8274365
Epoch: 035, Train Loss: 0.6148, Validation Loss: 0.5944
Total nodes: 8275175
Epoch: 036, Train Loss: 0.6148, Validation Loss: 0.5927
Total nodes: 8273505
Epoch: 037, Train Loss: 0.6147, Validation Loss: 0.5928
Total nodes: 8279641
Epoch: 038, Train Loss: 0.6148, Validation Loss: 0.5914
Total nodes: 8276889
Epoch: 039, Train Loss: 0.6147, Validation Loss: 0.5918
Total nodes: 8272959
Epoch: 040, Train Loss: 0.6147, Validation Loss: 0.5936
Total nodes: 8277344
Epoch: 041, Train Loss: 0.6147, Validation Loss: 0.5916
Total nodes: 8279533
Epoch: 042, Train Loss: 0.6147, Validation Loss: 0.5902
Total nodes: 8273054
Epoch: 043, Train Loss: 0.6146, Validation Loss: 0.5922
Total nodes: 8270609
Epoch: 044, Train Loss: 0.6146, Validation Loss: 0.5906
Total nodes: 8274629
Epoch: 045, Train Loss: 0.6146, Validation Loss: 0.5927
Total nodes: 8278102
Epoch: 046, Train Loss: 0.6146, Validation Loss: 0.5907
Total nodes: 8276979
Epoch: 047, Train Loss: 0.6146, Validation Loss: 0.5879
Total nodes: 8276276
Epoch: 048, Train Loss: 0.6146, Validation Loss: 0.5896
Total nodes: 8279255
Epoch: 049, Train Loss: 0.6146, Validation Loss: 0.5925
Total nodes: 8273974
Epoch: 050, Train Loss: 0.6145, Validation Loss: 0.5901
Total nodes: 8272059
Epoch: 051, Train Loss: 0.6145, Validation Loss: 0.5899
Total nodes: 8275080
Epoch: 052, Train Loss: 0.6145, Validation Loss: 0.5874
Total nodes: 8276057
Epoch: 053, Train Loss: 0.6145, Validation Loss: 0.5917
Total nodes: 8278767
Epoch: 054, Train Loss: 0.6145, Validation Loss: 0.5879
Total nodes: 8268236
Epoch: 055, Train Loss: 0.6145, Validation Loss: 0.5887
Total nodes: 8275288
Epoch: 056, Train Loss: 0.6144, Validation Loss: 0.5897
Total nodes: 8273884
Epoch: 057, Train Loss: 0.6145, Validation Loss: 0.5903
Total nodes: 8275713
Epoch: 058, Train Loss: 0.6144, Validation Loss: 0.5890
Total nodes: 8272971
Epoch: 059, Train Loss: 0.6144, Validation Loss: 0.5891
Total nodes: 8276382
Epoch: 060, Train Loss: 0.6144, Validation Loss: 0.5886
Total nodes: 8272926
Epoch: 061, Train Loss: 0.6144, Validation Loss: 0.5902
Total nodes: 8277092
Epoch: 062, Train Loss: 0.6144, Validation Loss: 0.5900
Total nodes: 8273639
Epoch: 063, Train Loss: 0.6144, Validation Loss: 0.5912
Total nodes: 8273923
Epoch: 064, Train Loss: 0.6144, Validation Loss: 0.5884
Total nodes: 8268942
Epoch: 065, Train Loss: 0.6144, Validation Loss: 0.5909
Total nodes: 8275226
Epoch: 066, Train Loss: 0.6143, Validation Loss: 0.5870
Total nodes: 8273203
Epoch: 067, Train Loss: 0.6143, Validation Loss: 0.5875
Total nodes: 8272700
Epoch: 068, Train Loss: 0.6143, Validation Loss: 0.5878
Total nodes: 8275265
Epoch: 069, Train Loss: 0.6143, Validation Loss: 0.5885
Total nodes: 8276210
Epoch: 070, Train Loss: 0.6143, Validation Loss: 0.5889
Total nodes: 8272773
Epoch: 071, Train Loss: 0.6143, Validation Loss: 0.5869
Total nodes: 8278520
Epoch: 072, Train Loss: 0.6143, Validation Loss: 0.5894
Total nodes: 8273966
Epoch: 073, Train Loss: 0.6143, Validation Loss: 0.5865
Total nodes: 8275525
Epoch: 074, Train Loss: 0.6143, Validation Loss: 0.5857
Total nodes: 8273228
Epoch: 075, Train Loss: 0.6142, Validation Loss: 0.5877
Total nodes: 8276555
Epoch: 076, Train Loss: 0.6142, Validation Loss: 0.5848
Total nodes: 8276920
Epoch: 077, Train Loss: 0.6142, Validation Loss: 0.5888
Total nodes: 8275230
Epoch: 078, Train Loss: 0.6143, Validation Loss: 0.5890
Total nodes: 8269426
Epoch: 079, Train Loss: 0.6142, Validation Loss: 0.5860
Total nodes: 8276877
Epoch: 080, Train Loss: 0.6142, Validation Loss: 0.5884
Total nodes: 8274463
Epoch: 081, Train Loss: 0.6142, Validation Loss: 0.5876
Total nodes: 8269556
Epoch: 082, Train Loss: 0.6142, Validation Loss: 0.5852
Total nodes: 8279481
Epoch: 083, Train Loss: 0.6142, Validation Loss: 0.5866
Total nodes: 8278432
Epoch: 084, Train Loss: 0.6142, Validation Loss: 0.5887
Total nodes: 8274518
Epoch: 085, Train Loss: 0.6142, Validation Loss: 0.5865
Total nodes: 8272702
Epoch: 086, Train Loss: 0.6142, Validation Loss: 0.5853
Total nodes: 8275003
Epoch: 087, Train Loss: 0.6142, Validation Loss: 0.5855
Total nodes: 8277296
Epoch: 088, Train Loss: 0.6141, Validation Loss: 0.5856
Total nodes: 8275195
Epoch: 089, Train Loss: 0.6141, Validation Loss: 0.5871
Total nodes: 8275412
Epoch: 090, Train Loss: 0.6142, Validation Loss: 0.5844
Total nodes: 8279386
Epoch: 091, Train Loss: 0.6141, Validation Loss: 0.5847
Total nodes: 8273916
Epoch: 092, Train Loss: 0.6141, Validation Loss: 0.5872
Total nodes: 8275385
Epoch: 093, Train Loss: 0.6141, Validation Loss: 0.5874
Total nodes: 8275084
Epoch: 094, Train Loss: 0.6142, Validation Loss: 0.5840
Total nodes: 8275055
Epoch: 095, Train Loss: 0.6141, Validation Loss: 0.5871
Total nodes: 8275657
Epoch: 096, Train Loss: 0.6142, Validation Loss: 0.5843
Total nodes: 8276056
Epoch: 097, Train Loss: 0.6141, Validation Loss: 0.5882
Total nodes: 8273489
Epoch: 098, Train Loss: 0.6141, Validation Loss: 0.5837
Total nodes: 8279407
Epoch: 099, Train Loss: 0.6141, Validation Loss: 0.5857
Total nodes: 8276289
Epoch: 100, Train Loss: 0.6141, Validation Loss: 0.5822
Total nodes: 8274943
Epoch: 101, Train Loss: 0.6141, Validation Loss: 0.5871
Total nodes: 8276948
Epoch: 102, Train Loss: 0.6141, Validation Loss: 0.5835
Total nodes: 8275110
Epoch: 103, Train Loss: 0.6141, Validation Loss: 0.5874
Total nodes: 8268842
Epoch: 104, Train Loss: 0.6141, Validation Loss: 0.5853
Total nodes: 8276410
Epoch: 105, Train Loss: 0.6141, Validation Loss: 0.5850
Total nodes: 8273784
Epoch: 106, Train Loss: 0.6140, Validation Loss: 0.5855
Total nodes: 8274913
Epoch: 107, Train Loss: 0.6141, Validation Loss: 0.5843
Total nodes: 8272249
Epoch: 108, Train Loss: 0.6140, Validation Loss: 0.5854
Total nodes: 8273848
Epoch: 109, Train Loss: 0.6140, Validation Loss: 0.5851
Total nodes: 8272425
Epoch: 110, Train Loss: 0.6140, Validation Loss: 0.5828
Total nodes: 8275324
Epoch: 111, Train Loss: 0.6141, Validation Loss: 0.5848
Total nodes: 8273481
Epoch: 112, Train Loss: 0.6141, Validation Loss: 0.5857
Total nodes: 8275490
Epoch: 113, Train Loss: 0.6140, Validation Loss: 0.5848
Total nodes: 8273321
Epoch: 114, Train Loss: 0.6140, Validation Loss: 0.5839
Total nodes: 8274479
Epoch: 115, Train Loss: 0.6140, Validation Loss: 0.5839
Total nodes: 8278763
Epoch: 116, Train Loss: 0.6141, Validation Loss: 0.5854
Total nodes: 8273500
Epoch: 117, Train Loss: 0.6140, Validation Loss: 0.5832
Total nodes: 8273041
Epoch: 118, Train Loss: 0.6140, Validation Loss: 0.5844
Total nodes: 8275758
Epoch: 119, Train Loss: 0.6140, Validation Loss: 0.5840
Total nodes: 8274727
Epoch: 120, Train Loss: 0.6140, Validation Loss: 0.5864
Total nodes: 8274318
Epoch: 121, Train Loss: 0.6140, Validation Loss: 0.5846
Total nodes: 8278635
Epoch: 122, Train Loss: 0.6140, Validation Loss: 0.5862
Total nodes: 8273301
Epoch: 123, Train Loss: 0.6140, Validation Loss: 0.5835
Total nodes: 8278230
Epoch: 124, Train Loss: 0.6140, Validation Loss: 0.5847
Total nodes: 8278712
Epoch: 125, Train Loss: 0.6140, Validation Loss: 0.5841
Total nodes: 8275321
Epoch: 126, Train Loss: 0.6140, Validation Loss: 0.5827
Total nodes: 8270541
Epoch: 127, Train Loss: 0.6140, Validation Loss: 0.5822
Total nodes: 8271723
Epoch: 128, Train Loss: 0.6139, Validation Loss: 0.5829
Total nodes: 8279979
Epoch: 129, Train Loss: 0.6140, Validation Loss: 0.5827
Total nodes: 8279423
Epoch: 130, Train Loss: 0.6140, Validation Loss: 0.5835
Total nodes: 8278014
Epoch: 131, Train Loss: 0.6140, Validation Loss: 0.5817
Total nodes: 8273198
Epoch: 132, Train Loss: 0.6139, Validation Loss: 0.5820
Total nodes: 8282213
Epoch: 133, Train Loss: 0.6140, Validation Loss: 0.5837
Total nodes: 8276668
Epoch: 134, Train Loss: 0.6139, Validation Loss: 0.5827
Total nodes: 8275736
Epoch: 135, Train Loss: 0.6140, Validation Loss: 0.5827
Total nodes: 8275942
Epoch: 136, Train Loss: 0.6139, Validation Loss: 0.5813
Total nodes: 8278631
Epoch: 137, Train Loss: 0.6139, Validation Loss: 0.5830
Total nodes: 8277926
Epoch: 138, Train Loss: 0.6140, Validation Loss: 0.5827
Total nodes: 8272019
Epoch: 139, Train Loss: 0.6139, Validation Loss: 0.5823
Total nodes: 8278848
Epoch: 140, Train Loss: 0.6139, Validation Loss: 0.5830
Total nodes: 8276761
Epoch: 141, Train Loss: 0.6139, Validation Loss: 0.5824
Total nodes: 8281549
Epoch: 142, Train Loss: 0.6139, Validation Loss: 0.5837
Total nodes: 8274258
Epoch: 143, Train Loss: 0.6139, Validation Loss: 0.5812
Total nodes: 8275113
Epoch: 144, Train Loss: 0.6140, Validation Loss: 0.5828
Total nodes: 8274156
Epoch: 145, Train Loss: 0.6139, Validation Loss: 0.5835
Total nodes: 8269068
Epoch: 146, Train Loss: 0.6139, Validation Loss: 0.5813
Total nodes: 8272736
Epoch: 147, Train Loss: 0.6139, Validation Loss: 0.5844
Total nodes: 8278520
Epoch: 148, Train Loss: 0.6139, Validation Loss: 0.5837
Total nodes: 8279324
Epoch: 149, Train Loss: 0.6139, Validation Loss: 0.5833
Total nodes: 8272201
Epoch: 150, Train Loss: 0.6139, Validation Loss: 0.5828
Total nodes: 8275675
Epoch: 151, Train Loss: 0.6139, Validation Loss: 0.5833
Total nodes: 8278601
Epoch: 152, Train Loss: 0.6139, Validation Loss: 0.5831
Total nodes: 8279052
Epoch: 153, Train Loss: 0.6139, Validation Loss: 0.5804
Total nodes: 8278800
Epoch: 154, Train Loss: 0.6139, Validation Loss: 0.5827
Total nodes: 8271811
Epoch: 155, Train Loss: 0.6139, Validation Loss: 0.5827
Total nodes: 8280781
Epoch: 156, Train Loss: 0.6139, Validation Loss: 0.5807
Total nodes: 8274052
Epoch: 157, Train Loss: 0.6138, Validation Loss: 0.5804
Total nodes: 8277324
Epoch: 158, Train Loss: 0.6139, Validation Loss: 0.5806
Total nodes: 8277193
Epoch: 159, Train Loss: 0.6139, Validation Loss: 0.5816
Total nodes: 8276311
Epoch: 160, Train Loss: 0.6139, Validation Loss: 0.5818
Total nodes: 8272871
Epoch: 161, Train Loss: 0.6139, Validation Loss: 0.5817
Total nodes: 8274837
Epoch: 162, Train Loss: 0.6139, Validation Loss: 0.5813
Total nodes: 8275258
Epoch: 163, Train Loss: 0.6139, Validation Loss: 0.5818
Total nodes: 8274202
Epoch: 164, Train Loss: 0.6138, Validation Loss: 0.5819
Total nodes: 8277301
Epoch: 165, Train Loss: 0.6139, Validation Loss: 0.5816
Total nodes: 8274668
Epoch: 166, Train Loss: 0.6139, Validation Loss: 0.5794
Total nodes: 8275509
Epoch: 167, Train Loss: 0.6139, Validation Loss: 0.5819
Total nodes: 8280483
Epoch: 168, Train Loss: 0.6139, Validation Loss: 0.5839
Total nodes: 8273756
Epoch: 169, Train Loss: 0.6139, Validation Loss: 0.5813
Total nodes: 8274867
Epoch: 170, Train Loss: 0.6138, Validation Loss: 0.5800
Total nodes: 8274910
Epoch: 171, Train Loss: 0.6139, Validation Loss: 0.5803
Total nodes: 8269476
Epoch: 172, Train Loss: 0.6138, Validation Loss: 0.5806
Total nodes: 8274211
Epoch: 173, Train Loss: 0.6138, Validation Loss: 0.5807
Total nodes: 8278187
Epoch: 174, Train Loss: 0.6139, Validation Loss: 0.5805
Total nodes: 8278006
Epoch: 175, Train Loss: 0.6139, Validation Loss: 0.5809
Total nodes: 8278332
Epoch: 176, Train Loss: 0.6139, Validation Loss: 0.5792
Total nodes: 8275932
Epoch: 177, Train Loss: 0.6138, Validation Loss: 0.5805
Total nodes: 8275493
Epoch: 178, Train Loss: 0.6138, Validation Loss: 0.5824
Total nodes: 8275050
Epoch: 179, Train Loss: 0.6138, Validation Loss: 0.5807
Total nodes: 8273676
Epoch: 180, Train Loss: 0.6138, Validation Loss: 0.5797
Total nodes: 8273562
Epoch: 181, Train Loss: 0.6138, Validation Loss: 0.5820
Total nodes: 8276806
Epoch: 182, Train Loss: 0.6139, Validation Loss: 0.5806
Total nodes: 8276664
Epoch: 183, Train Loss: 0.6138, Validation Loss: 0.5807
Total nodes: 8267788
Epoch: 184, Train Loss: 0.6138, Validation Loss: 0.5805
Total nodes: 8270315
Epoch: 185, Train Loss: 0.6138, Validation Loss: 0.5801
Total nodes: 8273395
Epoch: 186, Train Loss: 0.6139, Validation Loss: 0.5817
Total nodes: 8276417
Epoch: 187, Train Loss: 0.6138, Validation Loss: 0.5805
Total nodes: 8277436
Epoch: 188, Train Loss: 0.6138, Validation Loss: 0.5800
Total nodes: 8274110
Epoch: 189, Train Loss: 0.6138, Validation Loss: 0.5788
Total nodes: 8272859
Epoch: 190, Train Loss: 0.6139, Validation Loss: 0.5809
Total nodes: 8276039
Epoch: 191, Train Loss: 0.6138, Validation Loss: 0.5808
Total nodes: 8276589
Epoch: 192, Train Loss: 0.6138, Validation Loss: 0.5808
Total nodes: 8269108
Epoch: 193, Train Loss: 0.6138, Validation Loss: 0.5827
Total nodes: 8274695
Epoch: 194, Train Loss: 0.6138, Validation Loss: 0.5803
Total nodes: 8274934
Epoch: 195, Train Loss: 0.6138, Validation Loss: 0.5808
Total nodes: 8272513
Epoch: 196, Train Loss: 0.6138, Validation Loss: 0.5796
Total nodes: 8271659
Epoch: 197, Train Loss: 0.6138, Validation Loss: 0.5815
Total nodes: 8274562
Epoch: 198, Train Loss: 0.6138, Validation Loss: 0.5811
Total nodes: 8279570
Epoch: 199, Train Loss: 0.6138, Validation Loss: 0.5796
Total nodes: 8275461
Epoch: 200, Train Loss: 0.6138, Validation Loss: 0.5785
Total nodes: 8275934
Epoch: 201, Train Loss: 0.6138, Validation Loss: 0.5818
Total nodes: 8276913
Epoch: 202, Train Loss: 0.6138, Validation Loss: 0.5814
Total nodes: 8270033
Epoch: 203, Train Loss: 0.6138, Validation Loss: 0.5809
Total nodes: 8273624
Epoch: 204, Train Loss: 0.6138, Validation Loss: 0.5808
Total nodes: 8274917
Epoch: 205, Train Loss: 0.6138, Validation Loss: 0.5788
Total nodes: 8273422
Epoch: 206, Train Loss: 0.6138, Validation Loss: 0.5801
Total nodes: 8276610
Epoch: 207, Train Loss: 0.6138, Validation Loss: 0.5792
Total nodes: 8272800
Epoch: 208, Train Loss: 0.6138, Validation Loss: 0.5781
Total nodes: 8272920
Epoch: 209, Train Loss: 0.6138, Validation Loss: 0.5807
Total nodes: 8271156
Epoch: 210, Train Loss: 0.6138, Validation Loss: 0.5803
Total nodes: 8279651
Epoch: 211, Train Loss: 0.6138, Validation Loss: 0.5791
Total nodes: 8274133
Epoch: 212, Train Loss: 0.6138, Validation Loss: 0.5814
Total nodes: 8278204
Epoch: 213, Train Loss: 0.6138, Validation Loss: 0.5788
Total nodes: 8275610
Epoch: 214, Train Loss: 0.6138, Validation Loss: 0.5793
Total nodes: 8278392
Epoch: 215, Train Loss: 0.6138, Validation Loss: 0.5776
Total nodes: 8275855
Epoch: 216, Train Loss: 0.6138, Validation Loss: 0.5787
Total nodes: 8273268
Epoch: 217, Train Loss: 0.6138, Validation Loss: 0.5795
Total nodes: 8276582
Epoch: 218, Train Loss: 0.6138, Validation Loss: 0.5812
Total nodes: 8278271
Epoch: 219, Train Loss: 0.6138, Validation Loss: 0.5805
Total nodes: 8278563
Epoch: 220, Train Loss: 0.6138, Validation Loss: 0.5805
Total nodes: 8278869
Epoch: 221, Train Loss: 0.6138, Validation Loss: 0.5806
Total nodes: 8273436
Epoch: 222, Train Loss: 0.6138, Validation Loss: 0.5796
Total nodes: 8274148
Epoch: 223, Train Loss: 0.6138, Validation Loss: 0.5804
Total nodes: 8279038
Epoch: 224, Train Loss: 0.6138, Validation Loss: 0.5812
Total nodes: 8270816
Epoch: 225, Train Loss: 0.6138, Validation Loss: 0.5794
Total nodes: 8273033
Epoch: 226, Train Loss: 0.6138, Validation Loss: 0.5799
Total nodes: 8273381
Epoch: 227, Train Loss: 0.6138, Validation Loss: 0.5807
Total nodes: 8276120
Epoch: 228, Train Loss: 0.6138, Validation Loss: 0.5799
Total nodes: 8273646
Epoch: 229, Train Loss: 0.6138, Validation Loss: 0.5799
Total nodes: 8273858
Epoch: 230, Train Loss: 0.6138, Validation Loss: 0.5805
Total nodes: 8276765
Epoch: 231, Train Loss: 0.6138, Validation Loss: 0.5807
Total nodes: 8271191
Epoch: 232, Train Loss: 0.6138, Validation Loss: 0.5813
Total nodes: 8272298
Epoch: 233, Train Loss: 0.6138, Validation Loss: 0.5813
Total nodes: 8275055
Epoch: 234, Train Loss: 0.6138, Validation Loss: 0.5801
Total nodes: 8271163
Epoch: 235, Train Loss: 0.6137, Validation Loss: 0.5802
Total nodes: 8275025
Epoch: 236, Train Loss: 0.6138, Validation Loss: 0.5812
Total nodes: 8278112
Epoch: 237, Train Loss: 0.6138, Validation Loss: 0.5799
Total nodes: 8272536
Epoch: 238, Train Loss: 0.6138, Validation Loss: 0.5784
Total nodes: 8277554
Epoch: 239, Train Loss: 0.6137, Validation Loss: 0.5789
Total nodes: 8277998
Epoch: 240, Train Loss: 0.6138, Validation Loss: 0.5800
Total nodes: 8279733
Epoch: 241, Train Loss: 0.6138, Validation Loss: 0.5791
Total nodes: 8276577
Epoch: 242, Train Loss: 0.6138, Validation Loss: 0.5785
Total nodes: 8276243
Epoch: 243, Train Loss: 0.6137, Validation Loss: 0.5786
Total nodes: 8277664
Epoch: 244, Train Loss: 0.6137, Validation Loss: 0.5814
Total nodes: 8272633
Epoch: 245, Train Loss: 0.6137, Validation Loss: 0.5777
Total nodes: 8273367
Epoch: 246, Train Loss: 0.6138, Validation Loss: 0.5795
Total nodes: 8271056
Epoch: 247, Train Loss: 0.6137, Validation Loss: 0.5791
Total nodes: 8271538
Epoch: 248, Train Loss: 0.6137, Validation Loss: 0.5796
Total nodes: 8275532
Epoch: 249, Train Loss: 0.6138, Validation Loss: 0.5811
Total nodes: 8277857
Epoch: 250, Train Loss: 0.6137, Validation Loss: 0.5784
Total nodes: 8276970
Epoch: 251, Train Loss: 0.6138, Validation Loss: 0.5810
Total nodes: 8277705
Epoch: 252, Train Loss: 0.6138, Validation Loss: 0.5799
Total nodes: 8274444
Epoch: 253, Train Loss: 0.6138, Validation Loss: 0.5786
Total nodes: 8280527
Epoch: 254, Train Loss: 0.6138, Validation Loss: 0.5789
Total nodes: 8279980
Epoch: 255, Train Loss: 0.6138, Validation Loss: 0.5783
Total nodes: 8273214
Epoch: 256, Train Loss: 0.6138, Validation Loss: 0.5783
Total nodes: 8273230
Epoch: 257, Train Loss: 0.6137, Validation Loss: 0.5787
Total nodes: 8279791
Epoch: 258, Train Loss: 0.6138, Validation Loss: 0.5790
Total nodes: 8277433
Epoch: 259, Train Loss: 0.6138, Validation Loss: 0.5780
Total nodes: 8275065
Epoch: 260, Train Loss: 0.6138, Validation Loss: 0.5793
Total nodes: 8279728
Epoch: 261, Train Loss: 0.6138, Validation Loss: 0.5791
Total nodes: 8275130
Epoch: 262, Train Loss: 0.6137, Validation Loss: 0.5802
Total nodes: 8274281
Epoch: 263, Train Loss: 0.6137, Validation Loss: 0.5770
Total nodes: 8274211
Epoch: 264, Train Loss: 0.6137, Validation Loss: 0.5789
Total nodes: 8273541
Epoch: 265, Train Loss: 0.6138, Validation Loss: 0.5788
Total nodes: 8272431
Epoch: 266, Train Loss: 0.6137, Validation Loss: 0.5791
Total nodes: 8274930
Epoch: 267, Train Loss: 0.6137, Validation Loss: 0.5804
Total nodes: 8277472
Epoch: 268, Train Loss: 0.6138, Validation Loss: 0.5785
Total nodes: 8278292
Epoch: 269, Train Loss: 0.6137, Validation Loss: 0.5777
Total nodes: 8280849
Epoch: 270, Train Loss: 0.6138, Validation Loss: 0.5785
Total nodes: 8278281
Epoch: 271, Train Loss: 0.6138, Validation Loss: 0.5781
Total nodes: 8272131
Epoch: 272, Train Loss: 0.6137, Validation Loss: 0.5794
Total nodes: 8271486
Early stopping for train loss!
The best model: 243th epoch
Reading graph:   0%|          | 0/2040053 [00:00<?, ?it/s]Reading graph:   9%|▉         | 182921/2040053 [00:00<00:01, 1825199.55it/s]Reading graph:  19%|█▉        | 383906/2040053 [00:00<00:00, 1933668.88it/s]Reading graph:  28%|██▊       | 577283/2040053 [00:00<00:00, 1912127.33it/s]Reading graph:  38%|███▊      | 768529/2040053 [00:00<00:00, 1909768.62it/s]Reading graph:  47%|████▋     | 959525/2040053 [00:00<00:00, 1834753.91it/s]Reading graph:  58%|█████▊    | 1176589/2040053 [00:00<00:00, 1945519.88it/s]Reading graph:  67%|██████▋   | 1371789/2040053 [00:00<00:00, 1785401.35it/s]Reading graph:  76%|███████▌  | 1552889/2040053 [00:00<00:00, 1625828.89it/s]Reading graph:  84%|████████▍ | 1719109/2040053 [00:00<00:00, 1570098.85it/s]Reading graph:  94%|█████████▎| 1910676/2040053 [00:01<00:00, 1665149.18it/s]                                                                             