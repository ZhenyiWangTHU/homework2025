Reading graph:   0%|          | 0/10481042 [00:00<?, ?it/s]Reading graph:   2%|▏         | 224280/10481042 [00:00<00:04, 2242374.44it/s]Reading graph:   4%|▍         | 453524/10481042 [00:00<00:04, 2271670.83it/s]Reading graph:   6%|▋         | 680692/10481042 [00:00<00:04, 2097768.64it/s]Reading graph:   9%|▊         | 912832/10481042 [00:00<00:04, 2181793.43it/s]Reading graph:  11%|█▏        | 1181440/10481042 [00:00<00:03, 2357922.60it/s]Reading graph:  14%|█▎        | 1418629/10481042 [00:00<00:03, 2272100.29it/s]Reading graph:  16%|█▌        | 1688305/10481042 [00:00<00:03, 2405348.43it/s]Reading graph:  19%|█▊        | 1961308/10481042 [00:00<00:03, 2505804.07it/s]Reading graph:  21%|██▏       | 2239129/10481042 [00:00<00:03, 2589297.05it/s]Reading graph:  24%|██▍       | 2513152/10481042 [00:01<00:03, 2635196.02it/s]Reading graph:  27%|██▋       | 2782124/10481042 [00:01<00:02, 2651692.14it/s]Reading graph:  29%|██▉       | 3059992/10481042 [00:01<00:02, 2690020.28it/s]Reading graph:  32%|███▏      | 3337723/10481042 [00:01<00:02, 2716308.51it/s]Reading graph:  34%|███▍      | 3609656/10481042 [00:01<00:02, 2716596.93it/s]Reading graph:  37%|███▋      | 3885365/10481042 [00:01<00:02, 2728735.25it/s]Reading graph:  40%|███▉      | 4158389/10481042 [00:01<00:02, 2722824.95it/s]Reading graph:  42%|████▏     | 4430779/10481042 [00:01<00:02, 2714930.18it/s]Reading graph:  45%|████▍     | 4702351/10481042 [00:01<00:02, 2698788.24it/s]Reading graph:  47%|████▋     | 4972301/10481042 [00:01<00:02, 2672360.52it/s]Reading graph:  50%|████▉     | 5239629/10481042 [00:02<00:01, 2630166.80it/s]Reading graph:  53%|█████▎    | 5502814/10481042 [00:02<00:01, 2610232.36it/s]Reading graph:  55%|█████▍    | 5763953/10481042 [00:02<00:01, 2603174.19it/s]Reading graph:  58%|█████▊    | 6028019/10481042 [00:02<00:01, 2614217.18it/s]Reading graph:  60%|██████    | 6294448/10481042 [00:02<00:01, 2629042.43it/s]Reading graph:  63%|██████▎   | 6560243/10481042 [00:02<00:01, 2637609.25it/s]Reading graph:  65%|██████▌   | 6845573/10481042 [00:02<00:01, 2701937.82it/s]Reading graph:  68%|██████▊   | 7129611/10481042 [00:02<00:01, 2743264.63it/s]Reading graph:  71%|███████   | 7423359/10481042 [00:02<00:01, 2801332.06it/s]Reading graph:  74%|███████▎  | 7716722/10481042 [00:02<00:00, 2840900.58it/s]Reading graph:  76%|███████▋  | 8010077/10481042 [00:03<00:00, 2868612.97it/s]Reading graph:  79%|███████▉  | 8306339/10481042 [00:03<00:00, 2896746.45it/s]Reading graph:  82%|████████▏ | 8600009/10481042 [00:03<00:00, 2908675.59it/s]Reading graph:  85%|████████▍ | 8896080/10481042 [00:03<00:00, 2924241.77it/s]Reading graph:  88%|████████▊ | 9188520/10481042 [00:03<00:00, 2923104.99it/s]Reading graph:  90%|█████████ | 9484623/10481042 [00:03<00:00, 2934435.46it/s]Reading graph:  93%|█████████▎| 9778075/10481042 [00:03<00:00, 2927896.71it/s]Reading graph:  96%|█████████▌| 10070874/10481042 [00:03<00:00, 2925550.64it/s]Reading graph:  99%|█████████▉| 10366881/10481042 [00:03<00:00, 2935831.56it/s]                                                                               Reading graph:   0%|          | 0/2999755 [00:00<?, ?it/s]Reading graph:   5%|▍         | 149864/2999755 [00:00<00:01, 1498584.26it/s]Reading graph:  11%|█         | 331106/2999755 [00:00<00:01, 1681260.54it/s]Reading graph:  17%|█▋        | 516641/2999755 [00:00<00:01, 1760673.34it/s]Reading graph:  24%|██▍       | 721001/2999755 [00:00<00:01, 1866622.10it/s]Reading graph:  31%|███▏      | 941660/2999755 [00:00<00:01, 1988915.44it/s]Reading graph:  38%|███▊      | 1140549/2999755 [00:00<00:01, 1713560.15it/s]Reading graph:  44%|████▍     | 1334310/2999755 [00:00<00:00, 1779568.22it/s]Reading graph:  51%|█████     | 1516898/2999755 [00:00<00:00, 1718797.53it/s]Reading graph:  57%|█████▋    | 1714769/2999755 [00:00<00:00, 1665156.31it/s]Reading graph:  64%|██████▍   | 1929117/2999755 [00:01<00:00, 1663739.24it/s]Reading graph:  72%|███████▏  | 2170259/2999755 [00:01<00:00, 1714351.24it/s]Reading graph:  81%|████████▏ | 2440665/2999755 [00:01<00:00, 1972007.28it/s]Reading graph:  91%|█████████ | 2716040/2999755 [00:01<00:00, 2183916.03it/s]Reading graph: 100%|█████████▉| 2994029/2999755 [00:01<00:00, 2350258.77it/s]                                                                             Computing METIS partitioning...
Done!
Computing METIS partitioning...
Done!
Total nodes: 36507
Epoch: 001, Train Loss: 0.6938
Epoch: 001, Validation Loss: 0.6931
Total nodes: 36507
Epoch: 002, Train Loss: 0.6938
Epoch: 002, Validation Loss: 0.6927
Total nodes: 36507
Epoch: 003, Train Loss: 0.6927
Epoch: 003, Validation Loss: 0.6924
Total nodes: 36507
Epoch: 004, Train Loss: 0.6924
Epoch: 004, Validation Loss: 0.6921
Total nodes: 36507
Epoch: 005, Train Loss: 0.6921
Epoch: 005, Validation Loss: 0.6918
Total nodes: 36507
Epoch: 006, Train Loss: 0.6913
Epoch: 006, Validation Loss: 0.6914
Total nodes: 36507
Epoch: 007, Train Loss: 0.6903
Epoch: 007, Validation Loss: 0.6912
Total nodes: 36507
Epoch: 008, Train Loss: 0.6901
Epoch: 008, Validation Loss: 0.6907
Total nodes: 36507
Epoch: 009, Train Loss: 0.6899
Epoch: 009, Validation Loss: 0.6905
Total nodes: 36507
Epoch: 010, Train Loss: 0.6886
Epoch: 010, Validation Loss: 0.6899
Total nodes: 36507
Epoch: 011, Train Loss: 0.6883
Epoch: 011, Validation Loss: 0.6895
Total nodes: 36507
Epoch: 012, Train Loss: 0.6874
Epoch: 012, Validation Loss: 0.6891
Total nodes: 36507
Epoch: 013, Train Loss: 0.6865
Epoch: 013, Validation Loss: 0.6886
Total nodes: 36507
Epoch: 014, Train Loss: 0.6856
Epoch: 014, Validation Loss: 0.6881
Total nodes: 36507
Epoch: 015, Train Loss: 0.6842
Epoch: 015, Validation Loss: 0.6874
Total nodes: 36507
Epoch: 016, Train Loss: 0.6838
Epoch: 016, Validation Loss: 0.6865
Total nodes: 36507
Epoch: 017, Train Loss: 0.6835
Epoch: 017, Validation Loss: 0.6857
Total nodes: 36507
Epoch: 018, Train Loss: 0.6819
Epoch: 018, Validation Loss: 0.6848
Total nodes: 36507
Epoch: 019, Train Loss: 0.6813
Epoch: 019, Validation Loss: 0.6843
Total nodes: 36507
Epoch: 020, Train Loss: 0.6798
Epoch: 020, Validation Loss: 0.6832
Total nodes: 36507
Epoch: 021, Train Loss: 0.6784
Epoch: 021, Validation Loss: 0.6817
Total nodes: 36507
Epoch: 022, Train Loss: 0.6765
Epoch: 022, Validation Loss: 0.6815
Total nodes: 36507
Epoch: 023, Train Loss: 0.6733
Epoch: 023, Validation Loss: 0.6806
Total nodes: 36507
Epoch: 024, Train Loss: 0.6723
Epoch: 024, Validation Loss: 0.6795
Total nodes: 36507
Epoch: 025, Train Loss: 0.6693
Epoch: 025, Validation Loss: 0.6766
Total nodes: 36507
Epoch: 026, Train Loss: 0.6671
Epoch: 026, Validation Loss: 0.6761
Total nodes: 36507
Epoch: 027, Train Loss: 0.6663
Epoch: 027, Validation Loss: 0.6739
Total nodes: 36507
Epoch: 028, Train Loss: 0.6624
Epoch: 028, Validation Loss: 0.6723
Total nodes: 36507
Epoch: 029, Train Loss: 0.6615
Epoch: 029, Validation Loss: 0.6694
Total nodes: 36507
Epoch: 030, Train Loss: 0.6575
Epoch: 030, Validation Loss: 0.6672
Total nodes: 36507
Epoch: 031, Train Loss: 0.6526
Epoch: 031, Validation Loss: 0.6642
Total nodes: 36507
Epoch: 032, Train Loss: 0.6487
Epoch: 032, Validation Loss: 0.6619
Total nodes: 36507
Epoch: 033, Train Loss: 0.6506
Epoch: 033, Validation Loss: 0.6592
Total nodes: 36507
Epoch: 034, Train Loss: 0.6424
Epoch: 034, Validation Loss: 0.6552
Total nodes: 36507
Epoch: 035, Train Loss: 0.6383
Epoch: 035, Validation Loss: 0.6539
Total nodes: 36507
Epoch: 036, Train Loss: 0.6393
Epoch: 036, Validation Loss: 0.6479
Total nodes: 36507
Epoch: 037, Train Loss: 0.6255
Epoch: 037, Validation Loss: 0.6431
Total nodes: 36507
Epoch: 038, Train Loss: 0.6280
Epoch: 038, Validation Loss: 0.6409
Total nodes: 36507
Epoch: 039, Train Loss: 0.6212
Epoch: 039, Validation Loss: 0.6362
Total nodes: 36507
Epoch: 040, Train Loss: 0.6148
Epoch: 040, Validation Loss: 0.6379
Total nodes: 36507
Epoch: 041, Train Loss: 0.6061
Epoch: 041, Validation Loss: 0.6328
Total nodes: 36507
Epoch: 042, Train Loss: 0.6056
Epoch: 042, Validation Loss: 0.6264
Total nodes: 36507
Epoch: 043, Train Loss: 0.5979
Epoch: 043, Validation Loss: 0.6185
Total nodes: 36507
Epoch: 044, Train Loss: 0.5931
Epoch: 044, Validation Loss: 0.6184
Total nodes: 36507
Epoch: 045, Train Loss: 0.5850
Epoch: 045, Validation Loss: 0.6114
Total nodes: 36507
Epoch: 046, Train Loss: 0.5791
Epoch: 046, Validation Loss: 0.6075
Total nodes: 36507
Epoch: 047, Train Loss: 0.5763
Epoch: 047, Validation Loss: 0.5989
Total nodes: 36507
Epoch: 048, Train Loss: 0.5624
Epoch: 048, Validation Loss: 0.6015
Total nodes: 36507
Epoch: 049, Train Loss: 0.5596
Epoch: 049, Validation Loss: 0.5962
Total nodes: 36507
Epoch: 050, Train Loss: 0.5471
Epoch: 050, Validation Loss: 0.5827
Total nodes: 36507
Epoch: 051, Train Loss: 0.5482
Epoch: 051, Validation Loss: 0.5800
Total nodes: 36507
Epoch: 052, Train Loss: 0.5441
Epoch: 052, Validation Loss: 0.5783
Total nodes: 36507
Epoch: 053, Train Loss: 0.5399
Epoch: 053, Validation Loss: 0.5747
Total nodes: 36507
Epoch: 054, Train Loss: 0.5169
Epoch: 054, Validation Loss: 0.5662
Total nodes: 36507
Epoch: 055, Train Loss: 0.5081
Epoch: 055, Validation Loss: 0.5673
Total nodes: 36507
Epoch: 056, Train Loss: 0.5181
Epoch: 056, Validation Loss: 0.5616
Total nodes: 36507
Epoch: 057, Train Loss: 0.5066
Epoch: 057, Validation Loss: 0.5548
Total nodes: 36507
Epoch: 058, Train Loss: 0.4967
Epoch: 058, Validation Loss: 0.5534
Total nodes: 36507
Epoch: 059, Train Loss: 0.5089
Epoch: 059, Validation Loss: 0.5484
Total nodes: 36507
Epoch: 060, Train Loss: 0.5056
Epoch: 060, Validation Loss: 0.5467
Total nodes: 36507
Epoch: 061, Train Loss: 0.5005
Epoch: 061, Validation Loss: 0.5455
Total nodes: 36507
Epoch: 062, Train Loss: 0.4696
Epoch: 062, Validation Loss: 0.5425
Total nodes: 36507
Epoch: 063, Train Loss: 0.4690
Epoch: 063, Validation Loss: 0.5394
Total nodes: 36507
Epoch: 064, Train Loss: 0.4686
Epoch: 064, Validation Loss: 0.5430
Total nodes: 36507
Epoch: 065, Train Loss: 0.4678
Epoch: 065, Validation Loss: 0.5280
Total nodes: 36507
Epoch: 066, Train Loss: 0.4520
Epoch: 066, Validation Loss: 0.5301
Total nodes: 36507
Epoch: 067, Train Loss: 0.4625
Epoch: 067, Validation Loss: 0.5301
Total nodes: 36507
Epoch: 068, Train Loss: 0.4414
Epoch: 068, Validation Loss: 0.5271
Total nodes: 36507
Epoch: 069, Train Loss: 0.4357
Epoch: 069, Validation Loss: 0.5196
Total nodes: 36507
Epoch: 070, Train Loss: 0.4403
Epoch: 070, Validation Loss: 0.5167
Total nodes: 36507
Epoch: 071, Train Loss: 0.4196
Epoch: 071, Validation Loss: 0.5130
Total nodes: 36507
Epoch: 072, Train Loss: 0.4360
Epoch: 072, Validation Loss: 0.5148
Total nodes: 36507
Epoch: 073, Train Loss: 0.4242
Epoch: 073, Validation Loss: 0.5191
Total nodes: 36507
Epoch: 074, Train Loss: 0.4041
Epoch: 074, Validation Loss: 0.5105
Total nodes: 36507
Epoch: 075, Train Loss: 0.4247
Epoch: 075, Validation Loss: 0.5076
Total nodes: 36507
Epoch: 076, Train Loss: 0.4103
Epoch: 076, Validation Loss: 0.5029
Total nodes: 36507
Epoch: 077, Train Loss: 0.4038
Epoch: 077, Validation Loss: 0.5031
Total nodes: 36507
Epoch: 078, Train Loss: 0.4037
Epoch: 078, Validation Loss: 0.5018
Total nodes: 36507
Epoch: 079, Train Loss: 0.4009
Epoch: 079, Validation Loss: 0.5051
Total nodes: 36507
Epoch: 080, Train Loss: 0.4145
Epoch: 080, Validation Loss: 0.4993
Total nodes: 36507
Epoch: 081, Train Loss: 0.4259
Epoch: 081, Validation Loss: 0.4972
Total nodes: 36507
Epoch: 082, Train Loss: 0.3796
Epoch: 082, Validation Loss: 0.4996
Total nodes: 36507
Epoch: 083, Train Loss: 0.3927
Epoch: 083, Validation Loss: 0.5027
Total nodes: 36507
Epoch: 084, Train Loss: 0.3735
Epoch: 084, Validation Loss: 0.5000
Total nodes: 36507
Epoch: 085, Train Loss: 0.3833
Epoch: 085, Validation Loss: 0.5065
Total nodes: 36507
Epoch: 086, Train Loss: 0.3667
Epoch: 086, Validation Loss: 0.4930
Total nodes: 36507
Epoch: 087, Train Loss: 0.3608
Epoch: 087, Validation Loss: 0.4987
Total nodes: 36507
Epoch: 088, Train Loss: 0.3766
Epoch: 088, Validation Loss: 0.4936
Total nodes: 36507
Epoch: 089, Train Loss: 0.3624
Epoch: 089, Validation Loss: 0.4940
Total nodes: 36507
Epoch: 090, Train Loss: 0.3587
Epoch: 090, Validation Loss: 0.4883
Total nodes: 36507
Epoch: 091, Train Loss: 0.3456
Epoch: 091, Validation Loss: 0.4918
Total nodes: 36507
Epoch: 092, Train Loss: 0.3897
Epoch: 092, Validation Loss: 0.4827
Total nodes: 36507
Epoch: 093, Train Loss: 0.3605
Epoch: 093, Validation Loss: 0.4940
Total nodes: 36507
Epoch: 094, Train Loss: 0.3408
Epoch: 094, Validation Loss: 0.4916
Total nodes: 36507
Epoch: 095, Train Loss: 0.3852
Epoch: 095, Validation Loss: 0.4899
Total nodes: 36507
Epoch: 096, Train Loss: 0.3640
Epoch: 096, Validation Loss: 0.4911
Total nodes: 36507
Epoch: 097, Train Loss: 0.3390
Epoch: 097, Validation Loss: 0.4907
Total nodes: 36507
Epoch: 098, Train Loss: 0.3563
Epoch: 098, Validation Loss: 0.4886
Total nodes: 36507
Epoch: 099, Train Loss: 0.3230
Epoch: 099, Validation Loss: 0.4890
Total nodes: 36507
Epoch: 100, Train Loss: 0.3345
Epoch: 100, Validation Loss: 0.4973
Total nodes: 36507
Epoch: 101, Train Loss: 0.3131
Epoch: 101, Validation Loss: 0.4859
Total nodes: 36507
Epoch: 102, Train Loss: 0.3560
Epoch: 102, Validation Loss: 0.4831
Total nodes: 36507
Epoch: 103, Train Loss: 0.3207
Epoch: 103, Validation Loss: 0.4872
Total nodes: 36507
Epoch: 104, Train Loss: 0.3530
Epoch: 104, Validation Loss: 0.4962
Total nodes: 36507
Epoch: 105, Train Loss: 0.3334
Epoch: 105, Validation Loss: 0.4894
Total nodes: 36507
Epoch: 106, Train Loss: 0.3447
Epoch: 106, Validation Loss: 0.4895
Total nodes: 36507
Epoch: 107, Train Loss: 0.3297
Epoch: 107, Validation Loss: 0.4882
Total nodes: 36507
Epoch: 108, Train Loss: 0.3341
Epoch: 108, Validation Loss: 0.4855
Total nodes: 36507
Epoch: 109, Train Loss: 0.3226
Epoch: 109, Validation Loss: 0.4927
Total nodes: 36507
Epoch: 110, Train Loss: 0.3215
Epoch: 110, Validation Loss: 0.4848
Total nodes: 36507
Epoch: 111, Train Loss: 0.3278
Epoch: 111, Validation Loss: 0.4889
Total nodes: 36507
Epoch: 112, Train Loss: 0.3334
Epoch: 112, Validation Loss: 0.4900
Total nodes: 36507
Epoch: 113, Train Loss: 0.3495
Epoch: 113, Validation Loss: 0.4860
Total nodes: 36507
Epoch: 114, Train Loss: 0.3283
Epoch: 114, Validation Loss: 0.4928
Total nodes: 36507
Epoch: 115, Train Loss: 0.3233
Epoch: 115, Validation Loss: 0.4958
Total nodes: 36507
Epoch: 116, Train Loss: 0.3175
Epoch: 116, Validation Loss: 0.4874
Total nodes: 36507
Epoch: 117, Train Loss: 0.3013
Epoch: 117, Validation Loss: 0.4877
Total nodes: 36507
Epoch: 118, Train Loss: 0.3275
Epoch: 118, Validation Loss: 0.4945
Total nodes: 36507
Epoch: 119, Train Loss: 0.3301
Epoch: 119, Validation Loss: 0.4876
Total nodes: 36507
Epoch: 120, Train Loss: 0.3302
Epoch: 120, Validation Loss: 0.4918
Total nodes: 36507
Epoch: 121, Train Loss: 0.3189
Epoch: 121, Validation Loss: 0.4876
Total nodes: 36507
Epoch: 122, Train Loss: 0.3023
Epoch: 122, Validation Loss: 0.4873
Total nodes: 36507
Epoch: 123, Train Loss: 0.3071
Epoch: 123, Validation Loss: 0.4895
Total nodes: 36507
Epoch: 124, Train Loss: 0.3366
Epoch: 124, Validation Loss: 0.4888
Total nodes: 36507
Epoch: 125, Train Loss: 0.2979
Epoch: 125, Validation Loss: 0.4929
Total nodes: 36507
Epoch: 126, Train Loss: 0.2931
Epoch: 126, Validation Loss: 0.4945
Total nodes: 36507
Epoch: 127, Train Loss: 0.3324
Epoch: 127, Validation Loss: 0.4922
Total nodes: 36507
Epoch: 128, Train Loss: 0.2965
Epoch: 128, Validation Loss: 0.4889
Total nodes: 36507
Epoch: 129, Train Loss: 0.3122
Epoch: 129, Validation Loss: 0.4859
Total nodes: 36507
Epoch: 130, Train Loss: 0.3027
Epoch: 130, Validation Loss: 0.4847
Total nodes: 36507
Epoch: 131, Train Loss: 0.3177
Epoch: 131, Validation Loss: 0.4915
Total nodes: 36507
Epoch: 132, Train Loss: 0.3011
Epoch: 132, Validation Loss: 0.4929
Total nodes: 36507
Epoch: 133, Train Loss: 0.2914
Epoch: 133, Validation Loss: 0.4879
Total nodes: 36507
Epoch: 134, Train Loss: 0.3029
Epoch: 134, Validation Loss: 0.4909
Total nodes: 36507
Epoch: 135, Train Loss: 0.2955
Epoch: 135, Validation Loss: 0.4849
Total nodes: 36507
Epoch: 136, Train Loss: 0.2936
Epoch: 136, Validation Loss: 0.4890
Total nodes: 36507
Epoch: 137, Train Loss: 0.3138
Epoch: 137, Validation Loss: 0.4847
Total nodes: 36507
Epoch: 138, Train Loss: 0.3089
Epoch: 138, Validation Loss: 0.4923
Total nodes: 36507
Epoch: 139, Train Loss: 0.2840
Epoch: 139, Validation Loss: 0.4887
Total nodes: 36507
Epoch: 140, Train Loss: 0.3013
Epoch: 140, Validation Loss: 0.4852
Total nodes: 36507
Epoch: 141, Train Loss: 0.2812
Epoch: 141, Validation Loss: 0.4870
Total nodes: 36507
Epoch: 142, Train Loss: 0.2918
Epoch: 142, Validation Loss: 0.4887
Total nodes: 36507
Epoch: 143, Train Loss: 0.2864
Epoch: 143, Validation Loss: 0.4906
Total nodes: 36507
Epoch: 144, Train Loss: 0.2846
Epoch: 144, Validation Loss: 0.4930
Total nodes: 36507
Epoch: 145, Train Loss: 0.3002
Epoch: 145, Validation Loss: 0.4885
Total nodes: 36507
Epoch: 146, Train Loss: 0.3111
Epoch: 146, Validation Loss: 0.4936
Total nodes: 36507
Epoch: 147, Train Loss: 0.3148
Epoch: 147, Validation Loss: 0.4893
Total nodes: 36507
Epoch: 148, Train Loss: 0.2898
Epoch: 148, Validation Loss: 0.4915
Total nodes: 36507
Epoch: 149, Train Loss: 0.2974
Epoch: 149, Validation Loss: 0.4915
Total nodes: 36507
Epoch: 150, Train Loss: 0.2827
Epoch: 150, Validation Loss: 0.4835
Total nodes: 36507
Epoch: 151, Train Loss: 0.2793
Epoch: 151, Validation Loss: 0.4905
Total nodes: 36507
Epoch: 152, Train Loss: 0.2874
Epoch: 152, Validation Loss: 0.4937
Total nodes: 36507
Epoch: 153, Train Loss: 0.2746
Epoch: 153, Validation Loss: 0.4939
Total nodes: 36507
Epoch: 154, Train Loss: 0.2715
Epoch: 154, Validation Loss: 0.4879
Total nodes: 36507
Epoch: 155, Train Loss: 0.2682
Epoch: 155, Validation Loss: 0.4873
Total nodes: 36507
Epoch: 156, Train Loss: 0.3044
Epoch: 156, Validation Loss: 0.4916
Total nodes: 36507
Epoch: 157, Train Loss: 0.2799
Epoch: 157, Validation Loss: 0.4891
Total nodes: 36507
Epoch: 158, Train Loss: 0.2989
Epoch: 158, Validation Loss: 0.4957
Total nodes: 36507
Epoch: 159, Train Loss: 0.2649
Epoch: 159, Validation Loss: 0.4959
Total nodes: 36507
Epoch: 160, Train Loss: 0.2735
Epoch: 160, Validation Loss: 0.4878
Total nodes: 36507
Epoch: 161, Train Loss: 0.2896
Epoch: 161, Validation Loss: 0.4947
Total nodes: 36507
Epoch: 162, Train Loss: 0.2812
Epoch: 162, Validation Loss: 0.4839
Total nodes: 36507
Epoch: 163, Train Loss: 0.2987
Epoch: 163, Validation Loss: 0.4926
Total nodes: 36507
Epoch: 164, Train Loss: 0.2708
Epoch: 164, Validation Loss: 0.4938
Total nodes: 36507
Epoch: 165, Train Loss: 0.2715
Epoch: 165, Validation Loss: 0.4908
Total nodes: 36507
Epoch: 166, Train Loss: 0.3055
Epoch: 166, Validation Loss: 0.4968
Total nodes: 36507
Epoch: 167, Train Loss: 0.2828
Epoch: 167, Validation Loss: 0.4914
Total nodes: 36507
Epoch: 168, Train Loss: 0.2532
Epoch: 168, Validation Loss: 0.4887
Total nodes: 36507
Epoch: 169, Train Loss: 0.2687
Epoch: 169, Validation Loss: 0.4928
Total nodes: 36507
Epoch: 170, Train Loss: 0.2923
Epoch: 170, Validation Loss: 0.4859
Total nodes: 36507
Epoch: 171, Train Loss: 0.2927
Epoch: 171, Validation Loss: 0.4942
Total nodes: 36507
Epoch: 172, Train Loss: 0.2712
Epoch: 172, Validation Loss: 0.4905
Total nodes: 36507
Epoch: 173, Train Loss: 0.2663
Epoch: 173, Validation Loss: 0.4985
Total nodes: 36507
Epoch: 174, Train Loss: 0.2811
Epoch: 174, Validation Loss: 0.4980
Total nodes: 36507
Epoch: 175, Train Loss: 0.2623
Epoch: 175, Validation Loss: 0.4919
Total nodes: 36507
Epoch: 176, Train Loss: 0.2614
Epoch: 176, Validation Loss: 0.4912
Total nodes: 36507
Epoch: 177, Train Loss: 0.2813
Epoch: 177, Validation Loss: 0.4851
Total nodes: 36507
Epoch: 178, Train Loss: 0.2782
Epoch: 178, Validation Loss: 0.4905
Total nodes: 36507
Epoch: 179, Train Loss: 0.2641
Epoch: 179, Validation Loss: 0.4891
Total nodes: 36507
Epoch: 180, Train Loss: 0.2843
Epoch: 180, Validation Loss: 0.4931
Total nodes: 36507
Epoch: 181, Train Loss: 0.2765
Epoch: 181, Validation Loss: 0.4853
Total nodes: 36507
Epoch: 182, Train Loss: 0.2737
Epoch: 182, Validation Loss: 0.4976
Total nodes: 36507
Epoch: 183, Train Loss: 0.2712
Epoch: 183, Validation Loss: 0.4946
Total nodes: 36507
Epoch: 184, Train Loss: 0.2927
Epoch: 184, Validation Loss: 0.4969
Total nodes: 36507
Epoch: 185, Train Loss: 0.2686
Epoch: 185, Validation Loss: 0.4921
Total nodes: 36507
Epoch: 186, Train Loss: 0.2483
Epoch: 186, Validation Loss: 0.4804
Total nodes: 36507
Epoch: 187, Train Loss: 0.2771
Epoch: 187, Validation Loss: 0.4907
Total nodes: 36507
Epoch: 188, Train Loss: 0.2665
Epoch: 188, Validation Loss: 0.4886
Total nodes: 36507
Epoch: 189, Train Loss: 0.2740
Epoch: 189, Validation Loss: 0.4994
Total nodes: 36507
Epoch: 190, Train Loss: 0.2718
Epoch: 190, Validation Loss: 0.4861
Total nodes: 36507
Epoch: 191, Train Loss: 0.2583
Epoch: 191, Validation Loss: 0.4938
Total nodes: 36507
Epoch: 192, Train Loss: 0.2657
Epoch: 192, Validation Loss: 0.4838
Total nodes: 36507
Epoch: 193, Train Loss: 0.2705
Epoch: 193, Validation Loss: 0.4948
Total nodes: 36507
Epoch: 194, Train Loss: 0.2728
Epoch: 194, Validation Loss: 0.4992
Total nodes: 36507
Epoch: 195, Train Loss: 0.2651
Epoch: 195, Validation Loss: 0.4920
Total nodes: 36507
Epoch: 196, Train Loss: 0.2585
Epoch: 196, Validation Loss: 0.4927
Total nodes: 36507
Epoch: 197, Train Loss: 0.2563
Epoch: 197, Validation Loss: 0.4917
Total nodes: 36507
Epoch: 198, Train Loss: 0.2805
Epoch: 198, Validation Loss: 0.4941
Total nodes: 36507
Epoch: 199, Train Loss: 0.2585
Epoch: 199, Validation Loss: 0.4928
Total nodes: 36507
Epoch: 200, Train Loss: 0.2644
Epoch: 200, Validation Loss: 0.4935
Total nodes: 36507
Epoch: 201, Train Loss: 0.2747
Epoch: 201, Validation Loss: 0.4919
Total nodes: 36507
Epoch: 202, Train Loss: 0.2377
Epoch: 202, Validation Loss: 0.4952
Total nodes: 36507
Epoch: 203, Train Loss: 0.2594
Epoch: 203, Validation Loss: 0.4907
Total nodes: 36507
Epoch: 204, Train Loss: 0.2492
Epoch: 204, Validation Loss: 0.4931
Total nodes: 36507
Epoch: 205, Train Loss: 0.2620
Epoch: 205, Validation Loss: 0.4961
Total nodes: 36507
Epoch: 206, Train Loss: 0.2632
Epoch: 206, Validation Loss: 0.4966
Total nodes: 36507
Epoch: 207, Train Loss: 0.2469
Epoch: 207, Validation Loss: 0.4938
Total nodes: 36507
Epoch: 208, Train Loss: 0.2571
Epoch: 208, Validation Loss: 0.4947
Total nodes: 36507
Epoch: 209, Train Loss: 0.2413
Epoch: 209, Validation Loss: 0.4902
Total nodes: 36507
Epoch: 210, Train Loss: 0.2436
Epoch: 210, Validation Loss: 0.4924
Total nodes: 36507
Epoch: 211, Train Loss: 0.2845
Epoch: 211, Validation Loss: 0.4904
Total nodes: 36507
Epoch: 212, Train Loss: 0.2656
Epoch: 212, Validation Loss: 0.4931
Total nodes: 36507
Epoch: 213, Train Loss: 0.2783
Epoch: 213, Validation Loss: 0.4866
Total nodes: 36507
Epoch: 214, Train Loss: 0.2551
Epoch: 214, Validation Loss: 0.4917
Total nodes: 36507
Epoch: 215, Train Loss: 0.2664
Epoch: 215, Validation Loss: 0.4926
Total nodes: 36507
Epoch: 216, Train Loss: 0.2366
Epoch: 216, Validation Loss: 0.4865
Total nodes: 36507
Epoch: 217, Train Loss: 0.2450
Epoch: 217, Validation Loss: 0.4924
Total nodes: 36507
Epoch: 218, Train Loss: 0.2336
Epoch: 218, Validation Loss: 0.4860
Total nodes: 36507
Epoch: 219, Train Loss: 0.2572
Epoch: 219, Validation Loss: 0.4923
Total nodes: 36507
Epoch: 220, Train Loss: 0.2609
Epoch: 220, Validation Loss: 0.4953
Total nodes: 36507
Epoch: 221, Train Loss: 0.2420
Epoch: 221, Validation Loss: 0.4937
Total nodes: 36507
Epoch: 222, Train Loss: 0.2373
Epoch: 222, Validation Loss: 0.4935
Total nodes: 36507
Epoch: 223, Train Loss: 0.2262
Epoch: 223, Validation Loss: 0.4878
Total nodes: 36507
Epoch: 224, Train Loss: 0.2796
Epoch: 224, Validation Loss: 0.4897
Total nodes: 36507
Epoch: 225, Train Loss: 0.2511
Epoch: 225, Validation Loss: 0.4950
Total nodes: 36507
Epoch: 226, Train Loss: 0.2452
Epoch: 226, Validation Loss: 0.4879
Total nodes: 36507
Epoch: 227, Train Loss: 0.2479
Epoch: 227, Validation Loss: 0.4985
Total nodes: 36507
Epoch: 228, Train Loss: 0.2691
Epoch: 228, Validation Loss: 0.4933
Total nodes: 36507
Epoch: 229, Train Loss: 0.2270
Epoch: 229, Validation Loss: 0.4916
Total nodes: 36507
Epoch: 230, Train Loss: 0.2475
Epoch: 230, Validation Loss: 0.4838
Total nodes: 36507
Epoch: 231, Train Loss: 0.2285
Epoch: 231, Validation Loss: 0.4934
Total nodes: 36507
Epoch: 232, Train Loss: 0.2398
Epoch: 232, Validation Loss: 0.4913
Total nodes: 36507
Epoch: 233, Train Loss: 0.2294
Epoch: 233, Validation Loss: 0.4930
Total nodes: 36507
Epoch: 234, Train Loss: 0.2090
Epoch: 234, Validation Loss: 0.4884
Total nodes: 36507
Epoch: 235, Train Loss: 0.2286
Epoch: 235, Validation Loss: 0.4871
Total nodes: 36507
Epoch: 236, Train Loss: 0.2410
Epoch: 236, Validation Loss: 0.4926
Total nodes: 36507
Epoch: 237, Train Loss: 0.2298
Epoch: 237, Validation Loss: 0.4872
Total nodes: 36507
Epoch: 238, Train Loss: 0.2503
Epoch: 238, Validation Loss: 0.4902
Total nodes: 36507
Epoch: 239, Train Loss: 0.2196
Epoch: 239, Validation Loss: 0.4912
Total nodes: 36507
Epoch: 240, Train Loss: 0.2444
Epoch: 240, Validation Loss: 0.5028
Total nodes: 36507
Epoch: 241, Train Loss: 0.2336
Epoch: 241, Validation Loss: 0.4887
Total nodes: 36507
Epoch: 242, Train Loss: 0.2503
Epoch: 242, Validation Loss: 0.4952
Total nodes: 36507
Epoch: 243, Train Loss: 0.2476
Epoch: 243, Validation Loss: 0.4891
Total nodes: 36507
Epoch: 244, Train Loss: 0.2566
Epoch: 244, Validation Loss: 0.4980
Total nodes: 36507
Epoch: 245, Train Loss: 0.2370
Epoch: 245, Validation Loss: 0.4940
Total nodes: 36507
Epoch: 246, Train Loss: 0.2223
Epoch: 246, Validation Loss: 0.4880
Total nodes: 36507
Epoch: 247, Train Loss: 0.2189
Epoch: 247, Validation Loss: 0.4788
Total nodes: 36507
Epoch: 248, Train Loss: 0.2130
Epoch: 248, Validation Loss: 0.4910
Total nodes: 36507
Epoch: 249, Train Loss: 0.2429
Epoch: 249, Validation Loss: 0.4980
Total nodes: 36507
Epoch: 250, Train Loss: 0.2304
Epoch: 250, Validation Loss: 0.4945
Total nodes: 36507
Epoch: 251, Train Loss: 0.2429
Epoch: 251, Validation Loss: 0.4864
Total nodes: 36507
Epoch: 252, Train Loss: 0.2402
Epoch: 252, Validation Loss: 0.4888
Total nodes: 36507
Epoch: 253, Train Loss: 0.2308
Epoch: 253, Validation Loss: 0.4884
Total nodes: 36507
Epoch: 254, Train Loss: 0.2175
Epoch: 254, Validation Loss: 0.4816
Total nodes: 36507
Epoch: 255, Train Loss: 0.2225
Epoch: 255, Validation Loss: 0.4914
Total nodes: 36507
Epoch: 256, Train Loss: 0.2324
Epoch: 256, Validation Loss: 0.4929
Total nodes: 36507
Epoch: 257, Train Loss: 0.2575
Epoch: 257, Validation Loss: 0.4907
Total nodes: 36507
Epoch: 258, Train Loss: 0.2288
Epoch: 258, Validation Loss: 0.4879
Total nodes: 36507
Epoch: 259, Train Loss: 0.2439
Epoch: 259, Validation Loss: 0.4888
Total nodes: 36507
Epoch: 260, Train Loss: 0.2267
Epoch: 260, Validation Loss: 0.4957
Total nodes: 36507
Epoch: 261, Train Loss: 0.2360
Epoch: 261, Validation Loss: 0.4908
Total nodes: 36507
Epoch: 262, Train Loss: 0.2153
Epoch: 262, Validation Loss: 0.4931
Total nodes: 36507
Epoch: 263, Train Loss: 0.2316
Epoch: 263, Validation Loss: 0.4898
Total nodes: 36507
Epoch: 264, Train Loss: 0.2418
Epoch: 264, Validation Loss: 0.4857
Total nodes: 36507
Epoch: 265, Train Loss: 0.2411
Epoch: 265, Validation Loss: 0.4977
Total nodes: 36507
Epoch: 266, Train Loss: 0.2118
Epoch: 266, Validation Loss: 0.4910
Total nodes: 36507
Epoch: 267, Train Loss: 0.2359
Epoch: 267, Validation Loss: 0.4807
Total nodes: 36507
Epoch: 268, Train Loss: 0.2275
Epoch: 268, Validation Loss: 0.4871
Total nodes: 36507
Epoch: 269, Train Loss: 0.2339
Epoch: 269, Validation Loss: 0.4844
Total nodes: 36507
Epoch: 270, Train Loss: 0.2316
Epoch: 270, Validation Loss: 0.4864
Total nodes: 36507
Epoch: 271, Train Loss: 0.2049
Epoch: 271, Validation Loss: 0.4867
Total nodes: 36507
Epoch: 272, Train Loss: 0.2354
Epoch: 272, Validation Loss: 0.4849
Total nodes: 36507
Epoch: 273, Train Loss: 0.2167
Epoch: 273, Validation Loss: 0.4941
Total nodes: 36507
Epoch: 274, Train Loss: 0.2212
Epoch: 274, Validation Loss: 0.4885
Total nodes: 36507
Epoch: 275, Train Loss: 0.2417
Epoch: 275, Validation Loss: 0.4942
Total nodes: 36507
Epoch: 276, Train Loss: 0.2282
Epoch: 276, Validation Loss: 0.4992
Total nodes: 36507
Epoch: 277, Train Loss: 0.2252
Epoch: 277, Validation Loss: 0.4881
Total nodes: 36507
Epoch: 278, Train Loss: 0.2228
Epoch: 278, Validation Loss: 0.4984
Total nodes: 36507
Epoch: 279, Train Loss: 0.2472
Epoch: 279, Validation Loss: 0.4916
Total nodes: 36507
Epoch: 280, Train Loss: 0.2239
Epoch: 280, Validation Loss: 0.4893
Total nodes: 36507
Epoch: 281, Train Loss: 0.2575
Epoch: 281, Validation Loss: 0.4971
Total nodes: 36507
Epoch: 282, Train Loss: 0.2288
Epoch: 282, Validation Loss: 0.5021
Total nodes: 36507
Epoch: 283, Train Loss: 0.2277
Epoch: 283, Validation Loss: 0.4937
Total nodes: 36507
Epoch: 284, Train Loss: 0.2208
Epoch: 284, Validation Loss: 0.4841
Total nodes: 36507
Epoch: 285, Train Loss: 0.2295
Epoch: 285, Validation Loss: 0.4850
Total nodes: 36507
Epoch: 286, Train Loss: 0.2256
Epoch: 286, Validation Loss: 0.4940
Total nodes: 36507
Epoch: 287, Train Loss: 0.2123
Epoch: 287, Validation Loss: 0.4926
Total nodes: 36507
Epoch: 288, Train Loss: 0.2175
Epoch: 288, Validation Loss: 0.4884
Total nodes: 36507
Epoch: 289, Train Loss: 0.2097
Epoch: 289, Validation Loss: 0.4874
Total nodes: 36507
Epoch: 290, Train Loss: 0.2126
Epoch: 290, Validation Loss: 0.4859
Total nodes: 36507
Epoch: 291, Train Loss: 0.2063
Epoch: 291, Validation Loss: 0.4873
Total nodes: 36507
Epoch: 292, Train Loss: 0.2120
Epoch: 292, Validation Loss: 0.4873
Total nodes: 36507
Epoch: 293, Train Loss: 0.2131
Epoch: 293, Validation Loss: 0.4923
Total nodes: 36507
Epoch: 294, Train Loss: 0.2326
Epoch: 294, Validation Loss: 0.4924
Total nodes: 36507
Epoch: 295, Train Loss: 0.2025
Epoch: 295, Validation Loss: 0.4935
Total nodes: 36507
Epoch: 296, Train Loss: 0.2369
Epoch: 296, Validation Loss: 0.4980
Total nodes: 36507
Epoch: 297, Train Loss: 0.2072
Epoch: 297, Validation Loss: 0.4818
Total nodes: 36507
Epoch: 298, Train Loss: 0.2464
Epoch: 298, Validation Loss: 0.4854
Total nodes: 36507
Epoch: 299, Train Loss: 0.2118
Epoch: 299, Validation Loss: 0.4912
Total nodes: 36507
Epoch: 300, Train Loss: 0.2226
Epoch: 300, Validation Loss: 0.4916
Total nodes: 36507
Epoch: 301, Train Loss: 0.2326
Epoch: 301, Validation Loss: 0.4893
Total nodes: 36507
Epoch: 302, Train Loss: 0.2291
Epoch: 302, Validation Loss: 0.4931
Total nodes: 36507
Epoch: 303, Train Loss: 0.2183
Epoch: 303, Validation Loss: 0.4955
Total nodes: 36507
Epoch: 304, Train Loss: 0.2131
Epoch: 304, Validation Loss: 0.4979
Total nodes: 36507
Epoch: 305, Train Loss: 0.2069
Epoch: 305, Validation Loss: 0.4816
Total nodes: 36507
Epoch: 306, Train Loss: 0.2030
Epoch: 306, Validation Loss: 0.4925
Total nodes: 36507
Epoch: 307, Train Loss: 0.2219
Epoch: 307, Validation Loss: 0.4952
Total nodes: 36507
Epoch: 308, Train Loss: 0.2004
Epoch: 308, Validation Loss: 0.4885
Total nodes: 36507
Epoch: 309, Train Loss: 0.2183
Epoch: 309, Validation Loss: 0.4808
Total nodes: 36507
Epoch: 310, Train Loss: 0.2294
Epoch: 310, Validation Loss: 0.4912
Total nodes: 36507
Epoch: 311, Train Loss: 0.2064
Epoch: 311, Validation Loss: 0.4945
Total nodes: 36507
Epoch: 312, Train Loss: 0.2134
Epoch: 312, Validation Loss: 0.4900
Total nodes: 36507
Epoch: 313, Train Loss: 0.2044
Epoch: 313, Validation Loss: 0.4885
Total nodes: 36507
Epoch: 314, Train Loss: 0.1886
Epoch: 314, Validation Loss: 0.4792
Total nodes: 36507
Epoch: 315, Train Loss: 0.2121
Epoch: 315, Validation Loss: 0.4900
Total nodes: 36507
Epoch: 316, Train Loss: 0.2140
Epoch: 316, Validation Loss: 0.4879
Total nodes: 36507
Epoch: 317, Train Loss: 0.2299
Epoch: 317, Validation Loss: 0.4806
Total nodes: 36507
Epoch: 318, Train Loss: 0.2087
Epoch: 318, Validation Loss: 0.4834
Total nodes: 36507
Epoch: 319, Train Loss: 0.2028
Epoch: 319, Validation Loss: 0.4826
Total nodes: 36507
Epoch: 320, Train Loss: 0.2123
Epoch: 320, Validation Loss: 0.4885
Total nodes: 36507
Epoch: 321, Train Loss: 0.2272
Epoch: 321, Validation Loss: 0.4895
Total nodes: 36507
Epoch: 322, Train Loss: 0.2065
Epoch: 322, Validation Loss: 0.4892
Total nodes: 36507
Epoch: 323, Train Loss: 0.2094
Epoch: 323, Validation Loss: 0.4843
Total nodes: 36507
Epoch: 324, Train Loss: 0.2081
Epoch: 324, Validation Loss: 0.4868
Total nodes: 36507
Epoch: 325, Train Loss: 0.1974
Epoch: 325, Validation Loss: 0.4895
Total nodes: 36507
Epoch: 326, Train Loss: 0.2001
Epoch: 326, Validation Loss: 0.4839
Total nodes: 36507
Epoch: 327, Train Loss: 0.2047
Epoch: 327, Validation Loss: 0.4931
Total nodes: 36507
Epoch: 328, Train Loss: 0.2151
Epoch: 328, Validation Loss: 0.4824
Total nodes: 36507
Epoch: 329, Train Loss: 0.2057
Epoch: 329, Validation Loss: 0.4852
Total nodes: 36507
Epoch: 330, Train Loss: 0.1965
Epoch: 330, Validation Loss: 0.4769
Total nodes: 36507
Epoch: 331, Train Loss: 0.2109
Epoch: 331, Validation Loss: 0.4908
Total nodes: 36507
Epoch: 332, Train Loss: 0.2033
Epoch: 332, Validation Loss: 0.4802
Total nodes: 36507
Epoch: 333, Train Loss: 0.2046
Epoch: 333, Validation Loss: 0.4878
Total nodes: 36507
Epoch: 334, Train Loss: 0.2055
Epoch: 334, Validation Loss: 0.4971
Total nodes: 36507
Epoch: 335, Train Loss: 0.1963
Epoch: 335, Validation Loss: 0.4767
Total nodes: 36507
Epoch: 336, Train Loss: 0.1929
Epoch: 336, Validation Loss: 0.4860
Total nodes: 36507
Epoch: 337, Train Loss: 0.2069
Epoch: 337, Validation Loss: 0.4890
Total nodes: 36507
Epoch: 338, Train Loss: 0.2165
Epoch: 338, Validation Loss: 0.4863
Total nodes: 36507
Epoch: 339, Train Loss: 0.1925
Epoch: 339, Validation Loss: 0.4863
Total nodes: 36507
Epoch: 340, Train Loss: 0.2022
Epoch: 340, Validation Loss: 0.4840
Total nodes: 36507
Epoch: 341, Train Loss: 0.2132
Epoch: 341, Validation Loss: 0.4903
Total nodes: 36507
Epoch: 342, Train Loss: 0.2036
Epoch: 342, Validation Loss: 0.4891
Total nodes: 36507
Epoch: 343, Train Loss: 0.1916
Epoch: 343, Validation Loss: 0.4790
Total nodes: 36507
Epoch: 344, Train Loss: 0.2222
Epoch: 344, Validation Loss: 0.4792
Total nodes: 36507
Epoch: 345, Train Loss: 0.2333
Epoch: 345, Validation Loss: 0.4896
Total nodes: 36507
Epoch: 346, Train Loss: 0.1847
Epoch: 346, Validation Loss: 0.4919
Total nodes: 36507
Epoch: 347, Train Loss: 0.2104
Epoch: 347, Validation Loss: 0.4851
Total nodes: 36507
Epoch: 348, Train Loss: 0.2047
Epoch: 348, Validation Loss: 0.4922
Total nodes: 36507
Epoch: 349, Train Loss: 0.1960
Epoch: 349, Validation Loss: 0.4865
Total nodes: 36507
Epoch: 350, Train Loss: 0.2029
Epoch: 350, Validation Loss: 0.4861
Total nodes: 36507
Epoch: 351, Train Loss: 0.2345
Epoch: 351, Validation Loss: 0.4867
Total nodes: 36507
Epoch: 352, Train Loss: 0.2142
Epoch: 352, Validation Loss: 0.4898
Total nodes: 36507
Epoch: 353, Train Loss: 0.2020
Epoch: 353, Validation Loss: 0.4803
Total nodes: 36507
Epoch: 354, Train Loss: 0.1930
Epoch: 354, Validation Loss: 0.4817
Total nodes: 36507
Epoch: 355, Train Loss: 0.2078
Epoch: 355, Validation Loss: 0.4946
Total nodes: 36507
Epoch: 356, Train Loss: 0.1970
Epoch: 356, Validation Loss: 0.4850
Total nodes: 36507
Epoch: 357, Train Loss: 0.1998
Epoch: 357, Validation Loss: 0.4840
Total nodes: 36507
Epoch: 358, Train Loss: 0.2000
Epoch: 358, Validation Loss: 0.4840
Total nodes: 36507
Epoch: 359, Train Loss: 0.1921
Epoch: 359, Validation Loss: 0.4898
Total nodes: 36507
Epoch: 360, Train Loss: 0.1930
Epoch: 360, Validation Loss: 0.4820
Total nodes: 36507
Epoch: 361, Train Loss: 0.1842
Epoch: 361, Validation Loss: 0.4863
Total nodes: 36507
Epoch: 362, Train Loss: 0.2000
Epoch: 362, Validation Loss: 0.4855
Total nodes: 36507
Epoch: 363, Train Loss: 0.2251
Epoch: 363, Validation Loss: 0.4820
Total nodes: 36507
Epoch: 364, Train Loss: 0.1904
Epoch: 364, Validation Loss: 0.4834
Total nodes: 36507
Epoch: 365, Train Loss: 0.2040
Epoch: 365, Validation Loss: 0.4859
Total nodes: 36507
Epoch: 366, Train Loss: 0.1880
Epoch: 366, Validation Loss: 0.4815
Total nodes: 36507
Epoch: 367, Train Loss: 0.2255
Epoch: 367, Validation Loss: 0.4813
Total nodes: 36507
Epoch: 368, Train Loss: 0.1973
Epoch: 368, Validation Loss: 0.4891
Total nodes: 36507
Epoch: 369, Train Loss: 0.1998
Epoch: 369, Validation Loss: 0.4830
Total nodes: 36507
Epoch: 370, Train Loss: 0.1791
Epoch: 370, Validation Loss: 0.4829
Total nodes: 36507
Epoch: 371, Train Loss: 0.1831
Epoch: 371, Validation Loss: 0.4736
Total nodes: 36507
Epoch: 372, Train Loss: 0.2007
Epoch: 372, Validation Loss: 0.4827
Total nodes: 36507
Epoch: 373, Train Loss: 0.1827
Epoch: 373, Validation Loss: 0.4807
Total nodes: 36507
Epoch: 374, Train Loss: 0.1912
Epoch: 374, Validation Loss: 0.4847
Total nodes: 36507
Epoch: 375, Train Loss: 0.1918
Epoch: 375, Validation Loss: 0.4797
Total nodes: 36507
Epoch: 376, Train Loss: 0.2319
Epoch: 376, Validation Loss: 0.4849
Total nodes: 36507
Epoch: 377, Train Loss: 0.1751
Epoch: 377, Validation Loss: 0.4808
Total nodes: 36507
Epoch: 378, Train Loss: 0.2021
Epoch: 378, Validation Loss: 0.4840
Total nodes: 36507
Epoch: 379, Train Loss: 0.1994
Epoch: 379, Validation Loss: 0.4777
Total nodes: 36507
Epoch: 380, Train Loss: 0.2041
Epoch: 380, Validation Loss: 0.4868
Total nodes: 36507
Epoch: 381, Train Loss: 0.1964
Epoch: 381, Validation Loss: 0.4840
Total nodes: 36507
Epoch: 382, Train Loss: 0.1964
Epoch: 382, Validation Loss: 0.4894
Total nodes: 36507
Epoch: 383, Train Loss: 0.1895
Epoch: 383, Validation Loss: 0.4840
Total nodes: 36507
Epoch: 384, Train Loss: 0.1885
Epoch: 384, Validation Loss: 0.4820
Total nodes: 36507
Epoch: 385, Train Loss: 0.1758
Epoch: 385, Validation Loss: 0.4825
Total nodes: 36507
Epoch: 386, Train Loss: 0.2045
Epoch: 386, Validation Loss: 0.4748
Total nodes: 36507
Epoch: 387, Train Loss: 0.2056
Epoch: 387, Validation Loss: 0.4851
Total nodes: 36507
Epoch: 388, Train Loss: 0.1853
Epoch: 388, Validation Loss: 0.4805
Total nodes: 36507
Epoch: 389, Train Loss: 0.1935
Epoch: 389, Validation Loss: 0.4796
Total nodes: 36507
Epoch: 390, Train Loss: 0.1840
Epoch: 390, Validation Loss: 0.4719
Total nodes: 36507
Epoch: 391, Train Loss: 0.1856
Epoch: 391, Validation Loss: 0.4735
Total nodes: 36507
Epoch: 392, Train Loss: 0.1697
Epoch: 392, Validation Loss: 0.4748
Total nodes: 36507
Epoch: 393, Train Loss: 0.2005
Epoch: 393, Validation Loss: 0.4761
Total nodes: 36507
Epoch: 394, Train Loss: 0.1874
Epoch: 394, Validation Loss: 0.4797
Total nodes: 36507
Epoch: 395, Train Loss: 0.1750
Epoch: 395, Validation Loss: 0.4788
Total nodes: 36507
Epoch: 396, Train Loss: 0.1778
Epoch: 396, Validation Loss: 0.4775
Total nodes: 36507
Epoch: 397, Train Loss: 0.1748
Epoch: 397, Validation Loss: 0.4805
Total nodes: 36507
Epoch: 398, Train Loss: 0.1779
Epoch: 398, Validation Loss: 0.4769
Total nodes: 36507
Epoch: 399, Train Loss: 0.1853
Epoch: 399, Validation Loss: 0.4767
Total nodes: 36507
Epoch: 400, Train Loss: 0.1742
Epoch: 400, Validation Loss: 0.4802
Total nodes: 36507
Epoch: 401, Train Loss: 0.1958
Epoch: 401, Validation Loss: 0.4736
Total nodes: 36507
Epoch: 402, Train Loss: 0.1938
Epoch: 402, Validation Loss: 0.4830
Total nodes: 36507
Epoch: 403, Train Loss: 0.1829
Epoch: 403, Validation Loss: 0.4801
Total nodes: 36507
Epoch: 404, Train Loss: 0.1833
Epoch: 404, Validation Loss: 0.4702
Total nodes: 36507
Epoch: 405, Train Loss: 0.2241
Epoch: 405, Validation Loss: 0.4801
Total nodes: 36507
Epoch: 406, Train Loss: 0.1796
Epoch: 406, Validation Loss: 0.4776
Total nodes: 36507
Epoch: 407, Train Loss: 0.1728
Epoch: 407, Validation Loss: 0.4769
Total nodes: 36507
Epoch: 408, Train Loss: 0.1842
Epoch: 408, Validation Loss: 0.4797
Total nodes: 36507
Epoch: 409, Train Loss: 0.1977
Epoch: 409, Validation Loss: 0.4793
Total nodes: 36507
Epoch: 410, Train Loss: 0.2021
Epoch: 410, Validation Loss: 0.4803
Total nodes: 36507
Epoch: 411, Train Loss: 0.1782
Epoch: 411, Validation Loss: 0.4806
Total nodes: 36507
Epoch: 412, Train Loss: 0.1957
Epoch: 412, Validation Loss: 0.4764
Total nodes: 36507
Epoch: 413, Train Loss: 0.1878
Epoch: 413, Validation Loss: 0.4754
Total nodes: 36507
Epoch: 414, Train Loss: 0.1565
Epoch: 414, Validation Loss: 0.4752
Total nodes: 36507
Epoch: 415, Train Loss: 0.1717
Epoch: 415, Validation Loss: 0.4749
Total nodes: 36507
Epoch: 416, Train Loss: 0.1946
Epoch: 416, Validation Loss: 0.4730
Total nodes: 36507
Epoch: 417, Train Loss: 0.1817
Epoch: 417, Validation Loss: 0.4780
Total nodes: 36507
Epoch: 418, Train Loss: 0.1659
Epoch: 418, Validation Loss: 0.4678
Total nodes: 36507
Epoch: 419, Train Loss: 0.1895
Epoch: 419, Validation Loss: 0.4817
Total nodes: 36507
Epoch: 420, Train Loss: 0.1752
Epoch: 420, Validation Loss: 0.4732
Total nodes: 36507
Epoch: 421, Train Loss: 0.1962
Epoch: 421, Validation Loss: 0.4744
Total nodes: 36507
Epoch: 422, Train Loss: 0.2079
Epoch: 422, Validation Loss: 0.4814
Total nodes: 36507
Epoch: 423, Train Loss: 0.1749
Epoch: 423, Validation Loss: 0.4761
Total nodes: 36507
Epoch: 424, Train Loss: 0.1937
Epoch: 424, Validation Loss: 0.4739
Total nodes: 36507
Epoch: 425, Train Loss: 0.1674
Epoch: 425, Validation Loss: 0.4738
Total nodes: 36507
Epoch: 426, Train Loss: 0.1844
Epoch: 426, Validation Loss: 0.4777
Total nodes: 36507
Epoch: 427, Train Loss: 0.1688
Epoch: 427, Validation Loss: 0.4721
Total nodes: 36507
Epoch: 428, Train Loss: 0.1732
Epoch: 428, Validation Loss: 0.4671
Total nodes: 36507
Epoch: 429, Train Loss: 0.1796
Epoch: 429, Validation Loss: 0.4702
Total nodes: 36507
Epoch: 430, Train Loss: 0.1921
Epoch: 430, Validation Loss: 0.4672
Total nodes: 36507
Epoch: 431, Train Loss: 0.1786
Epoch: 431, Validation Loss: 0.4736
Total nodes: 36507
Epoch: 432, Train Loss: 0.1844
Epoch: 432, Validation Loss: 0.4833
Total nodes: 36507
Epoch: 433, Train Loss: 0.1888
Epoch: 433, Validation Loss: 0.4742
Total nodes: 36507
Epoch: 434, Train Loss: 0.2144
Epoch: 434, Validation Loss: 0.4757
Total nodes: 36507
Epoch: 435, Train Loss: 0.1759
Epoch: 435, Validation Loss: 0.4802
Total nodes: 36507
Epoch: 436, Train Loss: 0.1974
Epoch: 436, Validation Loss: 0.4762
Total nodes: 36507
Epoch: 437, Train Loss: 0.1819
Epoch: 437, Validation Loss: 0.4694
Total nodes: 36507
Epoch: 438, Train Loss: 0.1765
Epoch: 438, Validation Loss: 0.4763
Total nodes: 36507
Epoch: 439, Train Loss: 0.2053
Epoch: 439, Validation Loss: 0.4774
Total nodes: 36507
Epoch: 440, Train Loss: 0.1878
Epoch: 440, Validation Loss: 0.4711
Total nodes: 36507
Epoch: 441, Train Loss: 0.1807
Epoch: 441, Validation Loss: 0.4761
Total nodes: 36507
Epoch: 442, Train Loss: 0.1745
Epoch: 442, Validation Loss: 0.4752
Total nodes: 36507
Epoch: 443, Train Loss: 0.1742
Epoch: 443, Validation Loss: 0.4701
Total nodes: 36507
Epoch: 444, Train Loss: 0.1799
Epoch: 444, Validation Loss: 0.4701
Total nodes: 36507
Epoch: 445, Train Loss: 0.1708
Epoch: 445, Validation Loss: 0.4722
Total nodes: 36507
Epoch: 446, Train Loss: 0.1788
Epoch: 446, Validation Loss: 0.4741
Total nodes: 36507
Epoch: 447, Train Loss: 0.1951
Epoch: 447, Validation Loss: 0.4800
Total nodes: 36507
Epoch: 448, Train Loss: 0.1977
Epoch: 448, Validation Loss: 0.4653
Total nodes: 36507
Epoch: 449, Train Loss: 0.1685
Epoch: 449, Validation Loss: 0.4777
Total nodes: 36507
Epoch: 450, Train Loss: 0.1773
Epoch: 450, Validation Loss: 0.4723
Total nodes: 36507
Epoch: 451, Train Loss: 0.1792
Epoch: 451, Validation Loss: 0.4666
Total nodes: 36507
Epoch: 452, Train Loss: 0.1813
Epoch: 452, Validation Loss: 0.4634
Total nodes: 36507
Epoch: 453, Train Loss: 0.1730
Epoch: 453, Validation Loss: 0.4655
Total nodes: 36507
Epoch: 454, Train Loss: 0.1764
Epoch: 454, Validation Loss: 0.4614
Total nodes: 36507
Epoch: 455, Train Loss: 0.1667
Epoch: 455, Validation Loss: 0.4722
Total nodes: 36507
Epoch: 456, Train Loss: 0.1627
Epoch: 456, Validation Loss: 0.4646
Total nodes: 36507
Epoch: 457, Train Loss: 0.1731
Epoch: 457, Validation Loss: 0.4709
Total nodes: 36507
Epoch: 458, Train Loss: 0.1686
Epoch: 458, Validation Loss: 0.4779
Total nodes: 36507
Epoch: 459, Train Loss: 0.1668
Epoch: 459, Validation Loss: 0.4684
Total nodes: 36507
Epoch: 460, Train Loss: 0.1685
Epoch: 460, Validation Loss: 0.4686
Total nodes: 36507
Epoch: 461, Train Loss: 0.1845
Epoch: 461, Validation Loss: 0.4707
Total nodes: 36507
Epoch: 462, Train Loss: 0.1922
Epoch: 462, Validation Loss: 0.4720
Total nodes: 36507
Epoch: 463, Train Loss: 0.1822
Epoch: 463, Validation Loss: 0.4701
Total nodes: 36507
Epoch: 464, Train Loss: 0.1881
Epoch: 464, Validation Loss: 0.4738
Total nodes: 36507
Epoch: 465, Train Loss: 0.1818
Epoch: 465, Validation Loss: 0.4700
Total nodes: 36507
Epoch: 466, Train Loss: 0.1685
Epoch: 466, Validation Loss: 0.4706
Total nodes: 36507
Epoch: 467, Train Loss: 0.1528
Epoch: 467, Validation Loss: 0.4623
Total nodes: 36507
Epoch: 468, Train Loss: 0.1764
Epoch: 468, Validation Loss: 0.4691
Total nodes: 36507
Epoch: 469, Train Loss: 0.1860
Epoch: 469, Validation Loss: 0.4687
Total nodes: 36507
Epoch: 470, Train Loss: 0.1609
Epoch: 470, Validation Loss: 0.4624
Total nodes: 36507
Epoch: 471, Train Loss: 0.1744
Epoch: 471, Validation Loss: 0.4683
Total nodes: 36507
Epoch: 472, Train Loss: 0.1741
Epoch: 472, Validation Loss: 0.4652
Total nodes: 36507
Epoch: 473, Train Loss: 0.1665
Epoch: 473, Validation Loss: 0.4642
Total nodes: 36507
Epoch: 474, Train Loss: 0.1573
Epoch: 474, Validation Loss: 0.4715
Total nodes: 36507
Epoch: 475, Train Loss: 0.1622
Epoch: 475, Validation Loss: 0.4624
Total nodes: 36507
Epoch: 476, Train Loss: 0.1731
Epoch: 476, Validation Loss: 0.4643
Total nodes: 36507
Epoch: 477, Train Loss: 0.1591
Epoch: 477, Validation Loss: 0.4732
Total nodes: 36507
Epoch: 478, Train Loss: 0.1692
Epoch: 478, Validation Loss: 0.4661
Total nodes: 36507
Epoch: 479, Train Loss: 0.1929
Epoch: 479, Validation Loss: 0.4738
Total nodes: 36507
Epoch: 480, Train Loss: 0.1709
Epoch: 480, Validation Loss: 0.4711
Total nodes: 36507
Epoch: 481, Train Loss: 0.1789
Epoch: 481, Validation Loss: 0.4697
Total nodes: 36507
Epoch: 482, Train Loss: 0.1953
Epoch: 482, Validation Loss: 0.4684
Total nodes: 36507
Epoch: 483, Train Loss: 0.1693
Epoch: 483, Validation Loss: 0.4680
Total nodes: 36507
Epoch: 484, Train Loss: 0.1745
Epoch: 484, Validation Loss: 0.4661
Total nodes: 36507
Epoch: 485, Train Loss: 0.1795
Epoch: 485, Validation Loss: 0.4654
Total nodes: 36507
Epoch: 486, Train Loss: 0.1785
Epoch: 486, Validation Loss: 0.4661
Total nodes: 36507
Epoch: 487, Train Loss: 0.1566
Epoch: 487, Validation Loss: 0.4755
Total nodes: 36507
Epoch: 488, Train Loss: 0.1795
Epoch: 488, Validation Loss: 0.4650
Total nodes: 36507
Epoch: 489, Train Loss: 0.1507
Epoch: 489, Validation Loss: 0.4632
Total nodes: 36507
Epoch: 490, Train Loss: 0.1683
Epoch: 490, Validation Loss: 0.4618
Total nodes: 36507
Epoch: 491, Train Loss: 0.1622
Epoch: 491, Validation Loss: 0.4610
Total nodes: 36507
Epoch: 492, Train Loss: 0.1725
Epoch: 492, Validation Loss: 0.4724
Total nodes: 36507
Epoch: 493, Train Loss: 0.1788
Epoch: 493, Validation Loss: 0.4690
Total nodes: 36507
Epoch: 494, Train Loss: 0.1653
Epoch: 494, Validation Loss: 0.4655
Total nodes: 36507
Epoch: 495, Train Loss: 0.1751
Epoch: 495, Validation Loss: 0.4686
Total nodes: 36507
Epoch: 496, Train Loss: 0.1636
Epoch: 496, Validation Loss: 0.4587
Total nodes: 36507
Epoch: 497, Train Loss: 0.1784
Epoch: 497, Validation Loss: 0.4646
Total nodes: 36507
Epoch: 498, Train Loss: 0.1650
Epoch: 498, Validation Loss: 0.4624
Total nodes: 36507
Epoch: 499, Train Loss: 0.1864
Epoch: 499, Validation Loss: 0.4598
Total nodes: 36507
Epoch: 500, Train Loss: 0.1635
Epoch: 500, Validation Loss: 0.4599
Total nodes: 36507
Epoch: 501, Train Loss: 0.1863
Epoch: 501, Validation Loss: 0.4594
Total nodes: 36507
Epoch: 502, Train Loss: 0.1514
Epoch: 502, Validation Loss: 0.4575
Total nodes: 36507
Epoch: 503, Train Loss: 0.1601
Epoch: 503, Validation Loss: 0.4650
Total nodes: 36507
Epoch: 504, Train Loss: 0.1810
Epoch: 504, Validation Loss: 0.4692
Total nodes: 36507
Epoch: 505, Train Loss: 0.1583
Epoch: 505, Validation Loss: 0.4664
Total nodes: 36507
Epoch: 506, Train Loss: 0.1687
Epoch: 506, Validation Loss: 0.4641
Total nodes: 36507
Epoch: 507, Train Loss: 0.1639
Epoch: 507, Validation Loss: 0.4678
Total nodes: 36507
Epoch: 508, Train Loss: 0.1668
Epoch: 508, Validation Loss: 0.4634
Total nodes: 36507
Epoch: 509, Train Loss: 0.1664
Epoch: 509, Validation Loss: 0.4608
Total nodes: 36507
Epoch: 510, Train Loss: 0.1593
Epoch: 510, Validation Loss: 0.4575
Total nodes: 36507
Epoch: 511, Train Loss: 0.1593
Epoch: 511, Validation Loss: 0.4622
Total nodes: 36507
Epoch: 512, Train Loss: 0.1744
Epoch: 512, Validation Loss: 0.4566
Total nodes: 36507
Epoch: 513, Train Loss: 0.1584
Epoch: 513, Validation Loss: 0.4656
Total nodes: 36507
Epoch: 514, Train Loss: 0.1652
Epoch: 514, Validation Loss: 0.4677
Total nodes: 36507
Epoch: 515, Train Loss: 0.1686
Epoch: 515, Validation Loss: 0.4619
Total nodes: 36507
Epoch: 516, Train Loss: 0.1459
Epoch: 516, Validation Loss: 0.4590
Total nodes: 36507
Epoch: 517, Train Loss: 0.1728
Epoch: 517, Validation Loss: 0.4601
Total nodes: 36507
Epoch: 518, Train Loss: 0.1494
Epoch: 518, Validation Loss: 0.4680
Total nodes: 36507
Epoch: 519, Train Loss: 0.1631
Epoch: 519, Validation Loss: 0.4588
Total nodes: 36507
Epoch: 520, Train Loss: 0.1516
Epoch: 520, Validation Loss: 0.4641
Total nodes: 36507
Epoch: 521, Train Loss: 0.1540
Epoch: 521, Validation Loss: 0.4629
Total nodes: 36507
Epoch: 522, Train Loss: 0.1542
Epoch: 522, Validation Loss: 0.4549
Total nodes: 36507
Epoch: 523, Train Loss: 0.1524
Epoch: 523, Validation Loss: 0.4576
Total nodes: 36507
Epoch: 524, Train Loss: 0.1511
Epoch: 524, Validation Loss: 0.4553
Total nodes: 36507
Epoch: 525, Train Loss: 0.1568
Epoch: 525, Validation Loss: 0.4546
Total nodes: 36507
Epoch: 526, Train Loss: 0.1560
Epoch: 526, Validation Loss: 0.4606
Total nodes: 36507
Epoch: 527, Train Loss: 0.1722
Epoch: 527, Validation Loss: 0.4649
Total nodes: 36507
Epoch: 528, Train Loss: 0.1497
Epoch: 528, Validation Loss: 0.4604
Total nodes: 36507
Epoch: 529, Train Loss: 0.1537
Epoch: 529, Validation Loss: 0.4568
Total nodes: 36507
Epoch: 530, Train Loss: 0.1730
Epoch: 530, Validation Loss: 0.4638
Total nodes: 36507
Epoch: 531, Train Loss: 0.1575
Epoch: 531, Validation Loss: 0.4630
Total nodes: 36507
Epoch: 532, Train Loss: 0.1512
Epoch: 532, Validation Loss: 0.4619
Total nodes: 36507
Epoch: 533, Train Loss: 0.1484
Epoch: 533, Validation Loss: 0.4554
Total nodes: 36507
Epoch: 534, Train Loss: 0.1647
Epoch: 534, Validation Loss: 0.4592
Total nodes: 36507
Epoch: 535, Train Loss: 0.1805
Epoch: 535, Validation Loss: 0.4628
Total nodes: 36507
Epoch: 536, Train Loss: 0.1600
Epoch: 536, Validation Loss: 0.4586
Total nodes: 36507
Epoch: 537, Train Loss: 0.1525
Epoch: 537, Validation Loss: 0.4633
Total nodes: 36507
Epoch: 538, Train Loss: 0.1513
Epoch: 538, Validation Loss: 0.4607
Total nodes: 36507
Epoch: 539, Train Loss: 0.1783
Epoch: 539, Validation Loss: 0.4665
Total nodes: 36507
Epoch: 540, Train Loss: 0.1635
Epoch: 540, Validation Loss: 0.4648
Total nodes: 36507
Epoch: 541, Train Loss: 0.1514
Epoch: 541, Validation Loss: 0.4522
Total nodes: 36507
Epoch: 542, Train Loss: 0.1578
Epoch: 542, Validation Loss: 0.4550
Total nodes: 36507
Epoch: 543, Train Loss: 0.1578
Epoch: 543, Validation Loss: 0.4560
Total nodes: 36507
Epoch: 544, Train Loss: 0.1615
Epoch: 544, Validation Loss: 0.4535
Total nodes: 36507
Epoch: 545, Train Loss: 0.1499
Epoch: 545, Validation Loss: 0.4546
Total nodes: 36507
Epoch: 546, Train Loss: 0.1589
Epoch: 546, Validation Loss: 0.4587
Total nodes: 36507
Epoch: 547, Train Loss: 0.1499
Epoch: 547, Validation Loss: 0.4679
Total nodes: 36507
Epoch: 548, Train Loss: 0.1537
Epoch: 548, Validation Loss: 0.4481
Total nodes: 36507
Epoch: 549, Train Loss: 0.1499
Epoch: 549, Validation Loss: 0.4537
Total nodes: 36507
Epoch: 550, Train Loss: 0.1324
Epoch: 550, Validation Loss: 0.4462
Total nodes: 36507
Epoch: 551, Train Loss: 0.1684
Epoch: 551, Validation Loss: 0.4562
Total nodes: 36507
Epoch: 552, Train Loss: 0.1602
Epoch: 552, Validation Loss: 0.4573
Total nodes: 36507
Epoch: 553, Train Loss: 0.1415
Epoch: 553, Validation Loss: 0.4504
Total nodes: 36507
Epoch: 554, Train Loss: 0.1649
Epoch: 554, Validation Loss: 0.4548
Total nodes: 36507
Epoch: 555, Train Loss: 0.1491
Epoch: 555, Validation Loss: 0.4512
Total nodes: 36507
Epoch: 556, Train Loss: 0.1427
Epoch: 556, Validation Loss: 0.4417
Total nodes: 36507
Epoch: 557, Train Loss: 0.1641
Epoch: 557, Validation Loss: 0.4559
Total nodes: 36507
Epoch: 558, Train Loss: 0.1302
Epoch: 558, Validation Loss: 0.4499
Total nodes: 36507
Epoch: 559, Train Loss: 0.1477
Epoch: 559, Validation Loss: 0.4512
Total nodes: 36507
Epoch: 560, Train Loss: 0.1381
Epoch: 560, Validation Loss: 0.4468
Total nodes: 36507
Epoch: 561, Train Loss: 0.1657
Epoch: 561, Validation Loss: 0.4593
Total nodes: 36507
Epoch: 562, Train Loss: 0.1452
Epoch: 562, Validation Loss: 0.4537
Total nodes: 36507
Epoch: 563, Train Loss: 0.1662
Epoch: 563, Validation Loss: 0.4581
Total nodes: 36507
Epoch: 564, Train Loss: 0.1686
Epoch: 564, Validation Loss: 0.4590
Total nodes: 36507
Epoch: 565, Train Loss: 0.1530
Epoch: 565, Validation Loss: 0.4564
Total nodes: 36507
Epoch: 566, Train Loss: 0.1402
Epoch: 566, Validation Loss: 0.4501
Total nodes: 36507
Epoch: 567, Train Loss: 0.1367
Epoch: 567, Validation Loss: 0.4497
Total nodes: 36507
Epoch: 568, Train Loss: 0.1321
Epoch: 568, Validation Loss: 0.4499
Total nodes: 36507
Epoch: 569, Train Loss: 0.1612
Epoch: 569, Validation Loss: 0.4483
Total nodes: 36507
Epoch: 570, Train Loss: 0.1661
Epoch: 570, Validation Loss: 0.4579
Total nodes: 36507
Epoch: 571, Train Loss: 0.1426
Epoch: 571, Validation Loss: 0.4513
Total nodes: 36507
Epoch: 572, Train Loss: 0.1499
Epoch: 572, Validation Loss: 0.4493
Total nodes: 36507
Epoch: 573, Train Loss: 0.1501
Epoch: 573, Validation Loss: 0.4642
Total nodes: 36507
Epoch: 574, Train Loss: 0.1529
Epoch: 574, Validation Loss: 0.4541
Total nodes: 36507
Epoch: 575, Train Loss: 0.1659
Epoch: 575, Validation Loss: 0.4520
Total nodes: 36507
Epoch: 576, Train Loss: 0.1753
Epoch: 576, Validation Loss: 0.4611
Total nodes: 36507
Epoch: 577, Train Loss: 0.1430
Epoch: 577, Validation Loss: 0.4576
Total nodes: 36507
Epoch: 578, Train Loss: 0.1687
Epoch: 578, Validation Loss: 0.4533
Total nodes: 36507
Epoch: 579, Train Loss: 0.1331
Epoch: 579, Validation Loss: 0.4498
Total nodes: 36507
Epoch: 580, Train Loss: 0.1291
Epoch: 580, Validation Loss: 0.4490
Total nodes: 36507
Epoch: 581, Train Loss: 0.1570
Epoch: 581, Validation Loss: 0.4430
Total nodes: 36507
Epoch: 582, Train Loss: 0.1706
Epoch: 582, Validation Loss: 0.4521
Total nodes: 36507
Epoch: 583, Train Loss: 0.1652
Epoch: 583, Validation Loss: 0.4591
Total nodes: 36507
Epoch: 584, Train Loss: 0.1563
Epoch: 584, Validation Loss: 0.4528
Total nodes: 36507
Epoch: 585, Train Loss: 0.1483
Epoch: 585, Validation Loss: 0.4528
Total nodes: 36507
Epoch: 586, Train Loss: 0.1565
Epoch: 586, Validation Loss: 0.4485
Total nodes: 36507
Epoch: 587, Train Loss: 0.1450
Epoch: 587, Validation Loss: 0.4420
Total nodes: 36507
Epoch: 588, Train Loss: 0.1446
Epoch: 588, Validation Loss: 0.4501
Total nodes: 36507
Epoch: 589, Train Loss: 0.1677
Epoch: 589, Validation Loss: 0.4469
Total nodes: 36507
Epoch: 590, Train Loss: 0.1507
Epoch: 590, Validation Loss: 0.4452
Total nodes: 36507
Epoch: 591, Train Loss: 0.1407
Epoch: 591, Validation Loss: 0.4515
Total nodes: 36507
Epoch: 592, Train Loss: 0.1483
Epoch: 592, Validation Loss: 0.4520
Total nodes: 36507
Epoch: 593, Train Loss: 0.1736
Epoch: 593, Validation Loss: 0.4487
Total nodes: 36507
Epoch: 594, Train Loss: 0.1469
Epoch: 594, Validation Loss: 0.4459
Total nodes: 36507
Epoch: 595, Train Loss: 0.1474
Epoch: 595, Validation Loss: 0.4509
Total nodes: 36507
Epoch: 596, Train Loss: 0.1440
Epoch: 596, Validation Loss: 0.4436
Total nodes: 36507
Epoch: 597, Train Loss: 0.1391
Epoch: 597, Validation Loss: 0.4482
Total nodes: 36507
Epoch: 598, Train Loss: 0.1501
Epoch: 598, Validation Loss: 0.4461
Total nodes: 36507
Epoch: 599, Train Loss: 0.1381
Epoch: 599, Validation Loss: 0.4513
Total nodes: 36507
Epoch: 600, Train Loss: 0.1254
Epoch: 600, Validation Loss: 0.4453
Total nodes: 36507
Epoch: 601, Train Loss: 0.1384
Epoch: 601, Validation Loss: 0.4455
Total nodes: 36507
Epoch: 602, Train Loss: 0.1471
Epoch: 602, Validation Loss: 0.4522
Total nodes: 36507
Epoch: 603, Train Loss: 0.1537
Epoch: 603, Validation Loss: 0.4488
Total nodes: 36507
Epoch: 604, Train Loss: 0.1612
Epoch: 604, Validation Loss: 0.4551
Total nodes: 36507
Epoch: 605, Train Loss: 0.1314
Epoch: 605, Validation Loss: 0.4450
Total nodes: 36507
Epoch: 606, Train Loss: 0.1557
Epoch: 606, Validation Loss: 0.4500
Total nodes: 36507
Epoch: 607, Train Loss: 0.1513
Epoch: 607, Validation Loss: 0.4468
Total nodes: 36507
Epoch: 608, Train Loss: 0.1392
Epoch: 608, Validation Loss: 0.4478
Total nodes: 36507
Epoch: 609, Train Loss: 0.1437
Epoch: 609, Validation Loss: 0.4451
Total nodes: 36507
Epoch: 610, Train Loss: 0.1343
Epoch: 610, Validation Loss: 0.4451
Total nodes: 36507
Epoch: 611, Train Loss: 0.1312
Epoch: 611, Validation Loss: 0.4429
Total nodes: 36507
Epoch: 612, Train Loss: 0.1395
Epoch: 612, Validation Loss: 0.4354
Total nodes: 36507
Epoch: 613, Train Loss: 0.1415
Epoch: 613, Validation Loss: 0.4346
Total nodes: 36507
Epoch: 614, Train Loss: 0.1520
Epoch: 614, Validation Loss: 0.4515
Total nodes: 36507
Epoch: 615, Train Loss: 0.1553
Epoch: 615, Validation Loss: 0.4515
Total nodes: 36507
Epoch: 616, Train Loss: 0.1275
Epoch: 616, Validation Loss: 0.4392
Total nodes: 36507
Epoch: 617, Train Loss: 0.1451
Epoch: 617, Validation Loss: 0.4425
Total nodes: 36507
Epoch: 618, Train Loss: 0.1485
Epoch: 618, Validation Loss: 0.4421
Total nodes: 36507
Epoch: 619, Train Loss: 0.1445
Epoch: 619, Validation Loss: 0.4433
Total nodes: 36507
Epoch: 620, Train Loss: 0.1430
Epoch: 620, Validation Loss: 0.4443
Total nodes: 36507
Epoch: 621, Train Loss: 0.1431
Epoch: 621, Validation Loss: 0.4379
Total nodes: 36507
Epoch: 622, Train Loss: 0.1363
Epoch: 622, Validation Loss: 0.4452
Total nodes: 36507
Epoch: 623, Train Loss: 0.1569
Epoch: 623, Validation Loss: 0.4458
Total nodes: 36507
Epoch: 624, Train Loss: 0.1605
Epoch: 624, Validation Loss: 0.4593
Total nodes: 36507
Epoch: 625, Train Loss: 0.1332
Epoch: 625, Validation Loss: 0.4469
Total nodes: 36507
Epoch: 626, Train Loss: 0.1355
Epoch: 626, Validation Loss: 0.4393
Total nodes: 36507
Epoch: 627, Train Loss: 0.1715
Epoch: 627, Validation Loss: 0.4393
Total nodes: 36507
Epoch: 628, Train Loss: 0.1389
Epoch: 628, Validation Loss: 0.4438
Total nodes: 36507
Epoch: 629, Train Loss: 0.1607
Epoch: 629, Validation Loss: 0.4398
Total nodes: 36507
Epoch: 630, Train Loss: 0.1327
Epoch: 630, Validation Loss: 0.4447
Total nodes: 36507
Epoch: 631, Train Loss: 0.1367
Epoch: 631, Validation Loss: 0.4471
Total nodes: 36507
Epoch: 632, Train Loss: 0.1270
Epoch: 632, Validation Loss: 0.4451
Total nodes: 36507
Epoch: 633, Train Loss: 0.1417
Epoch: 633, Validation Loss: 0.4362
Total nodes: 36507
Epoch: 634, Train Loss: 0.1434
Epoch: 634, Validation Loss: 0.4442
Total nodes: 36507
Epoch: 635, Train Loss: 0.1317
Epoch: 635, Validation Loss: 0.4384
Total nodes: 36507
Epoch: 636, Train Loss: 0.1693
Epoch: 636, Validation Loss: 0.4488
Total nodes: 36507
Epoch: 637, Train Loss: 0.1246
Epoch: 637, Validation Loss: 0.4465
Total nodes: 36507
Epoch: 638, Train Loss: 0.1357
Epoch: 638, Validation Loss: 0.4392
Total nodes: 36507
Epoch: 639, Train Loss: 0.1542
Epoch: 639, Validation Loss: 0.4409
Total nodes: 36507
Epoch: 640, Train Loss: 0.1318
Epoch: 640, Validation Loss: 0.4483
Total nodes: 36507
Epoch: 641, Train Loss: 0.1572
Epoch: 641, Validation Loss: 0.4505
Total nodes: 36507
Epoch: 642, Train Loss: 0.1310
Epoch: 642, Validation Loss: 0.4382
Total nodes: 36507
Epoch: 643, Train Loss: 0.1467
Epoch: 643, Validation Loss: 0.4310
Total nodes: 36507
Epoch: 644, Train Loss: 0.1356
Epoch: 644, Validation Loss: 0.4443
Total nodes: 36507
Epoch: 645, Train Loss: 0.1325
Epoch: 645, Validation Loss: 0.4408
Total nodes: 36507
Epoch: 646, Train Loss: 0.1300
Epoch: 646, Validation Loss: 0.4370
Total nodes: 36507
Epoch: 647, Train Loss: 0.1254
Epoch: 647, Validation Loss: 0.4314
Total nodes: 36507
Epoch: 648, Train Loss: 0.1377
Epoch: 648, Validation Loss: 0.4373
Total nodes: 36507
Epoch: 649, Train Loss: 0.1335
Epoch: 649, Validation Loss: 0.4407
Total nodes: 36507
Epoch: 650, Train Loss: 0.1332
Epoch: 650, Validation Loss: 0.4447
Total nodes: 36507
Epoch: 651, Train Loss: 0.1518
Epoch: 651, Validation Loss: 0.4523
Total nodes: 36507
Epoch: 652, Train Loss: 0.1662
Epoch: 652, Validation Loss: 0.4454
Total nodes: 36507
Epoch: 653, Train Loss: 0.1276
Epoch: 653, Validation Loss: 0.4396
Total nodes: 36507
Epoch: 654, Train Loss: 0.1553
Epoch: 654, Validation Loss: 0.4428
Total nodes: 36507
Epoch: 655, Train Loss: 0.1385
Epoch: 655, Validation Loss: 0.4443
Total nodes: 36507
Epoch: 656, Train Loss: 0.1448
Epoch: 656, Validation Loss: 0.4376
Total nodes: 36507
Epoch: 657, Train Loss: 0.1415
Epoch: 657, Validation Loss: 0.4360
Total nodes: 36507
Epoch: 658, Train Loss: 0.1524
Epoch: 658, Validation Loss: 0.4396
Total nodes: 36507
Epoch: 659, Train Loss: 0.1278
Epoch: 659, Validation Loss: 0.4377
Total nodes: 36507
Epoch: 660, Train Loss: 0.1524
Epoch: 660, Validation Loss: 0.4363
Total nodes: 36507
Epoch: 661, Train Loss: 0.1574
Epoch: 661, Validation Loss: 0.4284
Total nodes: 36507
Epoch: 662, Train Loss: 0.1785
Epoch: 662, Validation Loss: 0.4353
Total nodes: 36507
Epoch: 663, Train Loss: 0.1172
Epoch: 663, Validation Loss: 0.4361
Total nodes: 36507
Epoch: 664, Train Loss: 0.1305
Epoch: 664, Validation Loss: 0.4445
Total nodes: 36507
Epoch: 665, Train Loss: 0.1392
Epoch: 665, Validation Loss: 0.4401
Total nodes: 36507
Epoch: 666, Train Loss: 0.1234
Epoch: 666, Validation Loss: 0.4324
Total nodes: 36507
Epoch: 667, Train Loss: 0.1238
Epoch: 667, Validation Loss: 0.4429
Total nodes: 36507
Epoch: 668, Train Loss: 0.1376
Epoch: 668, Validation Loss: 0.4396
Total nodes: 36507
Epoch: 669, Train Loss: 0.1289
Epoch: 669, Validation Loss: 0.4373
Total nodes: 36507
Epoch: 670, Train Loss: 0.1251
Epoch: 670, Validation Loss: 0.4345
Total nodes: 36507
Epoch: 671, Train Loss: 0.1347
Epoch: 671, Validation Loss: 0.4354
Total nodes: 36507
Epoch: 672, Train Loss: 0.1477
Epoch: 672, Validation Loss: 0.4417
Total nodes: 36507
Epoch: 673, Train Loss: 0.1361
Epoch: 673, Validation Loss: 0.4486
Total nodes: 36507
Epoch: 674, Train Loss: 0.1344
Epoch: 674, Validation Loss: 0.4445
Total nodes: 36507
Epoch: 675, Train Loss: 0.1274
Epoch: 675, Validation Loss: 0.4360
Total nodes: 36507
Epoch: 676, Train Loss: 0.1182
Epoch: 676, Validation Loss: 0.4345
Total nodes: 36507
Epoch: 677, Train Loss: 0.1410
Epoch: 677, Validation Loss: 0.4278
Total nodes: 36507
Epoch: 678, Train Loss: 0.1415
Epoch: 678, Validation Loss: 0.4314
Total nodes: 36507
Epoch: 679, Train Loss: 0.1366
Epoch: 679, Validation Loss: 0.4408
Total nodes: 36507
Epoch: 680, Train Loss: 0.1277
Epoch: 680, Validation Loss: 0.4371
Total nodes: 36507
Epoch: 681, Train Loss: 0.1326
Epoch: 681, Validation Loss: 0.4373
Total nodes: 36507
Epoch: 682, Train Loss: 0.1533
Epoch: 682, Validation Loss: 0.4334
Total nodes: 36507
Epoch: 683, Train Loss: 0.1342
Epoch: 683, Validation Loss: 0.4454
Total nodes: 36507
Epoch: 684, Train Loss: 0.1245
Epoch: 684, Validation Loss: 0.4426
Total nodes: 36507
Epoch: 685, Train Loss: 0.1436
Epoch: 685, Validation Loss: 0.4381
Total nodes: 36507
Epoch: 686, Train Loss: 0.1200
Epoch: 686, Validation Loss: 0.4408
Total nodes: 36507
Epoch: 687, Train Loss: 0.1256
Epoch: 687, Validation Loss: 0.4338
Total nodes: 36507
Epoch: 688, Train Loss: 0.1435
Epoch: 688, Validation Loss: 0.4436
Total nodes: 36507
Epoch: 689, Train Loss: 0.1546
Epoch: 689, Validation Loss: 0.4356
Total nodes: 36507
Epoch: 690, Train Loss: 0.1415
Epoch: 690, Validation Loss: 0.4426
Total nodes: 36507
Epoch: 691, Train Loss: 0.1289
Epoch: 691, Validation Loss: 0.4377
Total nodes: 36507
Epoch: 692, Train Loss: 0.1251
Epoch: 692, Validation Loss: 0.4360
Total nodes: 36507
Epoch: 693, Train Loss: 0.1230
Epoch: 693, Validation Loss: 0.4388
Total nodes: 36507
Epoch: 694, Train Loss: 0.1273
Epoch: 694, Validation Loss: 0.4345
Total nodes: 36507
Epoch: 695, Train Loss: 0.1244
Epoch: 695, Validation Loss: 0.4345
Total nodes: 36507
Epoch: 696, Train Loss: 0.1358
Epoch: 696, Validation Loss: 0.4348
Total nodes: 36507
Epoch: 697, Train Loss: 0.1527
Epoch: 697, Validation Loss: 0.4348
Total nodes: 36507
Epoch: 698, Train Loss: 0.1306
Epoch: 698, Validation Loss: 0.4384
Total nodes: 36507
Epoch: 699, Train Loss: 0.1354
Epoch: 699, Validation Loss: 0.4429
Total nodes: 36507
Epoch: 700, Train Loss: 0.1327
Epoch: 700, Validation Loss: 0.4377
Total nodes: 36507
Epoch: 701, Train Loss: 0.1603
Epoch: 701, Validation Loss: 0.4499
Total nodes: 36507
Epoch: 702, Train Loss: 0.1172
Epoch: 702, Validation Loss: 0.4295
Total nodes: 36507
Epoch: 703, Train Loss: 0.1315
Epoch: 703, Validation Loss: 0.4381
Total nodes: 36507
Epoch: 704, Train Loss: 0.1359
Epoch: 704, Validation Loss: 0.4281
Total nodes: 36507
Epoch: 705, Train Loss: 0.1310
Epoch: 705, Validation Loss: 0.4417
Total nodes: 36507
Epoch: 706, Train Loss: 0.1058
Epoch: 706, Validation Loss: 0.4385
Total nodes: 36507
Epoch: 707, Train Loss: 0.1165
Epoch: 707, Validation Loss: 0.4372
Total nodes: 36507
Epoch: 708, Train Loss: 0.1219
Epoch: 708, Validation Loss: 0.4365
Total nodes: 36507
Epoch: 709, Train Loss: 0.1298
Epoch: 709, Validation Loss: 0.4417
Total nodes: 36507
Epoch: 710, Train Loss: 0.1164
Epoch: 710, Validation Loss: 0.4454
Total nodes: 36507
Epoch: 711, Train Loss: 0.1223
Epoch: 711, Validation Loss: 0.4365
Total nodes: 36507
Epoch: 712, Train Loss: 0.1262
Epoch: 712, Validation Loss: 0.4348
Total nodes: 36507
Epoch: 713, Train Loss: 0.1286
Epoch: 713, Validation Loss: 0.4352
Total nodes: 36507
Epoch: 714, Train Loss: 0.1285
Epoch: 714, Validation Loss: 0.4426
Total nodes: 36507
Epoch: 715, Train Loss: 0.1380
Epoch: 715, Validation Loss: 0.4464
Total nodes: 36507
Epoch: 716, Train Loss: 0.1481
Epoch: 716, Validation Loss: 0.4444
Total nodes: 36507
Epoch: 717, Train Loss: 0.1291
Epoch: 717, Validation Loss: 0.4410
Total nodes: 36507
Epoch: 718, Train Loss: 0.1120
Epoch: 718, Validation Loss: 0.4295
Total nodes: 36507
Epoch: 719, Train Loss: 0.1299
Epoch: 719, Validation Loss: 0.4263
Total nodes: 36507
Epoch: 720, Train Loss: 0.1174
Epoch: 720, Validation Loss: 0.4294
Total nodes: 36507
Epoch: 721, Train Loss: 0.1347
Epoch: 721, Validation Loss: 0.4342
Total nodes: 36507
Epoch: 722, Train Loss: 0.1277
Epoch: 722, Validation Loss: 0.4337
Total nodes: 36507
Epoch: 723, Train Loss: 0.1153
Epoch: 723, Validation Loss: 0.4361
Total nodes: 36507
Epoch: 724, Train Loss: 0.1164
Epoch: 724, Validation Loss: 0.4326
Total nodes: 36507
Epoch: 725, Train Loss: 0.1327
Epoch: 725, Validation Loss: 0.4306
Total nodes: 36507
Epoch: 726, Train Loss: 0.1246
Epoch: 726, Validation Loss: 0.4416
Total nodes: 36507
Epoch: 727, Train Loss: 0.1214
Epoch: 727, Validation Loss: 0.4416
Total nodes: 36507
Epoch: 728, Train Loss: 0.1329
Epoch: 728, Validation Loss: 0.4333
Total nodes: 36507
Epoch: 729, Train Loss: 0.1246
Epoch: 729, Validation Loss: 0.4367
Total nodes: 36507
Epoch: 730, Train Loss: 0.1088
Epoch: 730, Validation Loss: 0.4303
Total nodes: 36507
Epoch: 731, Train Loss: 0.1317
Epoch: 731, Validation Loss: 0.4322
Total nodes: 36507
Epoch: 732, Train Loss: 0.1174
Epoch: 732, Validation Loss: 0.4356
Total nodes: 36507
Epoch: 733, Train Loss: 0.1499
Epoch: 733, Validation Loss: 0.4311
Total nodes: 36507
Epoch: 734, Train Loss: 0.1243
Epoch: 734, Validation Loss: 0.4352
Total nodes: 36507
Epoch: 735, Train Loss: 0.1307
Epoch: 735, Validation Loss: 0.4415
Total nodes: 36507
Epoch: 736, Train Loss: 0.1304
Epoch: 736, Validation Loss: 0.4332
Total nodes: 36507
Epoch: 737, Train Loss: 0.1340
Epoch: 737, Validation Loss: 0.4365
Total nodes: 36507
Epoch: 738, Train Loss: 0.1307
Epoch: 738, Validation Loss: 0.4445
Total nodes: 36507
Epoch: 739, Train Loss: 0.1275
Epoch: 739, Validation Loss: 0.4346
Total nodes: 36507
Epoch: 740, Train Loss: 0.1305
Epoch: 740, Validation Loss: 0.4436
Total nodes: 36507
Epoch: 741, Train Loss: 0.1317
Epoch: 741, Validation Loss: 0.4432
Total nodes: 36507
Epoch: 742, Train Loss: 0.1390
Epoch: 742, Validation Loss: 0.4375
Total nodes: 36507
Epoch: 743, Train Loss: 0.1272
Epoch: 743, Validation Loss: 0.4333
Total nodes: 36507
Epoch: 744, Train Loss: 0.1330
Epoch: 744, Validation Loss: 0.4397
Total nodes: 36507
Epoch: 745, Train Loss: 0.1263
Epoch: 745, Validation Loss: 0.4396
Total nodes: 36507
Epoch: 746, Train Loss: 0.1299
Epoch: 746, Validation Loss: 0.4392
Total nodes: 36507
Epoch: 747, Train Loss: 0.1390
Epoch: 747, Validation Loss: 0.4360
Total nodes: 36507
Epoch: 748, Train Loss: 0.1207
Epoch: 748, Validation Loss: 0.4295
Total nodes: 36507
Epoch: 749, Train Loss: 0.1182
Epoch: 749, Validation Loss: 0.4344
Total nodes: 36507
Epoch: 750, Train Loss: 0.1402
Epoch: 750, Validation Loss: 0.4391
Total nodes: 36507
Epoch: 751, Train Loss: 0.1028
Epoch: 751, Validation Loss: 0.4318
Total nodes: 36507
Epoch: 752, Train Loss: 0.1239
Epoch: 752, Validation Loss: 0.4311
Total nodes: 36507
Epoch: 753, Train Loss: 0.1307
Epoch: 753, Validation Loss: 0.4349
Total nodes: 36507
Epoch: 754, Train Loss: 0.1251
Epoch: 754, Validation Loss: 0.4322
Total nodes: 36507
Epoch: 755, Train Loss: 0.1233
Epoch: 755, Validation Loss: 0.4249
Total nodes: 36507
Epoch: 756, Train Loss: 0.1213
Epoch: 756, Validation Loss: 0.4282
Total nodes: 36507
Epoch: 757, Train Loss: 0.1442
Epoch: 757, Validation Loss: 0.4383
Total nodes: 36507
Epoch: 758, Train Loss: 0.1404
Epoch: 758, Validation Loss: 0.4417
Total nodes: 36507
Epoch: 759, Train Loss: 0.1460
Epoch: 759, Validation Loss: 0.4304
Total nodes: 36507
Epoch: 760, Train Loss: 0.1335
Epoch: 760, Validation Loss: 0.4369
Total nodes: 36507
Epoch: 761, Train Loss: 0.1305
Epoch: 761, Validation Loss: 0.4335
Total nodes: 36507
Epoch: 762, Train Loss: 0.1045
Epoch: 762, Validation Loss: 0.4324
Total nodes: 36507
Epoch: 763, Train Loss: 0.1233
Epoch: 763, Validation Loss: 0.4426
Total nodes: 36507
Epoch: 764, Train Loss: 0.1305
Epoch: 764, Validation Loss: 0.4396
Total nodes: 36507
Epoch: 765, Train Loss: 0.1435
Epoch: 765, Validation Loss: 0.4369
Total nodes: 36507
Epoch: 766, Train Loss: 0.1109
Epoch: 766, Validation Loss: 0.4435
Total nodes: 36507
Epoch: 767, Train Loss: 0.1419
Epoch: 767, Validation Loss: 0.4372
Total nodes: 36507
Epoch: 768, Train Loss: 0.1269
Epoch: 768, Validation Loss: 0.4423
Total nodes: 36507
Epoch: 769, Train Loss: 0.1208
Epoch: 769, Validation Loss: 0.4331
Total nodes: 36507
Epoch: 770, Train Loss: 0.1256
Epoch: 770, Validation Loss: 0.4388
Total nodes: 36507
Epoch: 771, Train Loss: 0.1378
Epoch: 771, Validation Loss: 0.4424
Total nodes: 36507
Epoch: 772, Train Loss: 0.1268
Epoch: 772, Validation Loss: 0.4425
Total nodes: 36507
Epoch: 773, Train Loss: 0.1453
Epoch: 773, Validation Loss: 0.4347
Total nodes: 36507
Epoch: 774, Train Loss: 0.1128
Epoch: 774, Validation Loss: 0.4381
Total nodes: 36507
Epoch: 775, Train Loss: 0.1170
Epoch: 775, Validation Loss: 0.4295
Total nodes: 36507
Epoch: 776, Train Loss: 0.1152
Epoch: 776, Validation Loss: 0.4303
Total nodes: 36507
Epoch: 777, Train Loss: 0.1085
Epoch: 777, Validation Loss: 0.4293
Total nodes: 36507
Epoch: 778, Train Loss: 0.1196
Epoch: 778, Validation Loss: 0.4381
Total nodes: 36507
Epoch: 779, Train Loss: 0.1409
Epoch: 779, Validation Loss: 0.4307
Total nodes: 36507
Epoch: 780, Train Loss: 0.1227
Epoch: 780, Validation Loss: 0.4374
Total nodes: 36507
Epoch: 781, Train Loss: 0.1137
Epoch: 781, Validation Loss: 0.4246
Total nodes: 36507
Epoch: 782, Train Loss: 0.1098
Epoch: 782, Validation Loss: 0.4283
Total nodes: 36507
Epoch: 783, Train Loss: 0.1114
Epoch: 783, Validation Loss: 0.4289
Total nodes: 36507
Epoch: 784, Train Loss: 0.1247
Epoch: 784, Validation Loss: 0.4333
Total nodes: 36507
Epoch: 785, Train Loss: 0.1130
Epoch: 785, Validation Loss: 0.4350
Total nodes: 36507
Epoch: 786, Train Loss: 0.1239
Epoch: 786, Validation Loss: 0.4353
Total nodes: 36507
Epoch: 787, Train Loss: 0.0960
Epoch: 787, Validation Loss: 0.4318
Total nodes: 36507
Epoch: 788, Train Loss: 0.1395
Epoch: 788, Validation Loss: 0.4351
Total nodes: 36507
Epoch: 789, Train Loss: 0.1270
Epoch: 789, Validation Loss: 0.4365
Total nodes: 36507
Epoch: 790, Train Loss: 0.1085
Epoch: 790, Validation Loss: 0.4393
Total nodes: 36507
Epoch: 791, Train Loss: 0.1220
Epoch: 791, Validation Loss: 0.4389
Total nodes: 36507
Epoch: 792, Train Loss: 0.1035
Epoch: 792, Validation Loss: 0.4298
Total nodes: 36507
Epoch: 793, Train Loss: 0.1261
Epoch: 793, Validation Loss: 0.4309
Total nodes: 36507
Epoch: 794, Train Loss: 0.1055
Epoch: 794, Validation Loss: 0.4350
Total nodes: 36507
Epoch: 795, Train Loss: 0.1208
Epoch: 795, Validation Loss: 0.4316
Total nodes: 36507
Epoch: 796, Train Loss: 0.1316
Epoch: 796, Validation Loss: 0.4365
Total nodes: 36507
Epoch: 797, Train Loss: 0.1167
Epoch: 797, Validation Loss: 0.4330
Total nodes: 36507
Epoch: 798, Train Loss: 0.1127
Epoch: 798, Validation Loss: 0.4446
Total nodes: 36507
Epoch: 799, Train Loss: 0.1174
Epoch: 799, Validation Loss: 0.4321
Total nodes: 36507
Epoch: 800, Train Loss: 0.1308
Epoch: 800, Validation Loss: 0.4363
Total nodes: 36507
Epoch: 801, Train Loss: 0.1339
Epoch: 801, Validation Loss: 0.4336
Total nodes: 36507
Epoch: 802, Train Loss: 0.1292
Epoch: 802, Validation Loss: 0.4279
Total nodes: 36507
Epoch: 803, Train Loss: 0.1221
Epoch: 803, Validation Loss: 0.4346
Total nodes: 36507
Epoch: 804, Train Loss: 0.1149
Epoch: 804, Validation Loss: 0.4379
Total nodes: 36507
Epoch: 805, Train Loss: 0.1142
Epoch: 805, Validation Loss: 0.4328
Total nodes: 36507
Epoch: 806, Train Loss: 0.1240
Epoch: 806, Validation Loss: 0.4336
Total nodes: 36507
Epoch: 807, Train Loss: 0.1159
Epoch: 807, Validation Loss: 0.4299
Total nodes: 36507
Epoch: 808, Train Loss: 0.1240
Epoch: 808, Validation Loss: 0.4423
Total nodes: 36507
Epoch: 809, Train Loss: 0.1222
Epoch: 809, Validation Loss: 0.4390
Total nodes: 36507
Epoch: 810, Train Loss: 0.0951
Epoch: 810, Validation Loss: 0.4348
Total nodes: 36507
Epoch: 811, Train Loss: 0.1260
Epoch: 811, Validation Loss: 0.4320
Total nodes: 36507
Epoch: 812, Train Loss: 0.1074
Epoch: 812, Validation Loss: 0.4307
Total nodes: 36507
Epoch: 813, Train Loss: 0.1055
Epoch: 813, Validation Loss: 0.4366
Total nodes: 36507
Epoch: 814, Train Loss: 0.1260
Epoch: 814, Validation Loss: 0.4281
Total nodes: 36507
Epoch: 815, Train Loss: 0.1158
Epoch: 815, Validation Loss: 0.4370
Total nodes: 36507
Epoch: 816, Train Loss: 0.1141
Epoch: 816, Validation Loss: 0.4355
Total nodes: 36507
Epoch: 817, Train Loss: 0.1256
Epoch: 817, Validation Loss: 0.4375
Total nodes: 36507
Epoch: 818, Train Loss: 0.1142
Epoch: 818, Validation Loss: 0.4297
Total nodes: 36507
Epoch: 819, Train Loss: 0.1330
Epoch: 819, Validation Loss: 0.4337
Total nodes: 36507
Epoch: 820, Train Loss: 0.1287
Epoch: 820, Validation Loss: 0.4324
Total nodes: 36507
Epoch: 821, Train Loss: 0.1052
Epoch: 821, Validation Loss: 0.4343
Total nodes: 36507
Epoch: 822, Train Loss: 0.1106
Epoch: 822, Validation Loss: 0.4232
Total nodes: 36507
Epoch: 823, Train Loss: 0.1248
Epoch: 823, Validation Loss: 0.4340
Total nodes: 36507
Epoch: 824, Train Loss: 0.1031
Epoch: 824, Validation Loss: 0.4321
Total nodes: 36507
Epoch: 825, Train Loss: 0.1047
Epoch: 825, Validation Loss: 0.4286
Total nodes: 36507
Epoch: 826, Train Loss: 0.1065
Epoch: 826, Validation Loss: 0.4290
Total nodes: 36507
Epoch: 827, Train Loss: 0.1132
Epoch: 827, Validation Loss: 0.4361
Total nodes: 36507
Epoch: 828, Train Loss: 0.1132
Epoch: 828, Validation Loss: 0.4347
Total nodes: 36507
Epoch: 829, Train Loss: 0.1098
Epoch: 829, Validation Loss: 0.4356
Total nodes: 36507
Epoch: 830, Train Loss: 0.1237
Epoch: 830, Validation Loss: 0.4365
Total nodes: 36507
Epoch: 831, Train Loss: 0.1242
Epoch: 831, Validation Loss: 0.4375
Total nodes: 36507
Epoch: 832, Train Loss: 0.1303
Epoch: 832, Validation Loss: 0.4350
Total nodes: 36507
Epoch: 833, Train Loss: 0.1047
Epoch: 833, Validation Loss: 0.4321
Total nodes: 36507
Epoch: 834, Train Loss: 0.1112
Epoch: 834, Validation Loss: 0.4379
Total nodes: 36507
Epoch: 835, Train Loss: 0.1068
Epoch: 835, Validation Loss: 0.4401
Total nodes: 36507
Epoch: 836, Train Loss: 0.0928
Epoch: 836, Validation Loss: 0.4283
Total nodes: 36507
Epoch: 837, Train Loss: 0.0995
Epoch: 837, Validation Loss: 0.4301
Total nodes: 36507
Epoch: 838, Train Loss: 0.1125
Epoch: 838, Validation Loss: 0.4342
Total nodes: 36507
Epoch: 839, Train Loss: 0.1096
Epoch: 839, Validation Loss: 0.4378
Total nodes: 36507
Epoch: 840, Train Loss: 0.1176
Epoch: 840, Validation Loss: 0.4401
Total nodes: 36507
Epoch: 841, Train Loss: 0.1268
Epoch: 841, Validation Loss: 0.4388
Total nodes: 36507
Epoch: 842, Train Loss: 0.1085
Epoch: 842, Validation Loss: 0.4265
Total nodes: 36507
Epoch: 843, Train Loss: 0.1214
Epoch: 843, Validation Loss: 0.4338
Total nodes: 36507
Epoch: 844, Train Loss: 0.1064
Epoch: 844, Validation Loss: 0.4281
Total nodes: 36507
Epoch: 845, Train Loss: 0.1170
Epoch: 845, Validation Loss: 0.4259
Total nodes: 36507
Epoch: 846, Train Loss: 0.1135
Epoch: 846, Validation Loss: 0.4311
Total nodes: 36507
Epoch: 847, Train Loss: 0.1153
Epoch: 847, Validation Loss: 0.4309
Total nodes: 36507
Epoch: 848, Train Loss: 0.1159
Epoch: 848, Validation Loss: 0.4429
Total nodes: 36507
Epoch: 849, Train Loss: 0.1001
Epoch: 849, Validation Loss: 0.4333
Total nodes: 36507
Epoch: 850, Train Loss: 0.1176
Epoch: 850, Validation Loss: 0.4314
Total nodes: 36507
Epoch: 851, Train Loss: 0.1356
Epoch: 851, Validation Loss: 0.4358
Total nodes: 36507
Epoch: 852, Train Loss: 0.1029
Epoch: 852, Validation Loss: 0.4483
Total nodes: 36507
Epoch: 853, Train Loss: 0.1168
Epoch: 853, Validation Loss: 0.4331
Total nodes: 36507
Epoch: 854, Train Loss: 0.1052
Epoch: 854, Validation Loss: 0.4292
Total nodes: 36507
Epoch: 855, Train Loss: 0.1038
Epoch: 855, Validation Loss: 0.4364
Total nodes: 36507
Epoch: 856, Train Loss: 0.1149
Epoch: 856, Validation Loss: 0.4353
Total nodes: 36507
Epoch: 857, Train Loss: 0.1177
Epoch: 857, Validation Loss: 0.4280
Total nodes: 36507
Epoch: 858, Train Loss: 0.1034
Epoch: 858, Validation Loss: 0.4295
Total nodes: 36507
Epoch: 859, Train Loss: 0.1088
Epoch: 859, Validation Loss: 0.4323
Total nodes: 36507
Epoch: 860, Train Loss: 0.1128
Epoch: 860, Validation Loss: 0.4329
Total nodes: 36507
Epoch: 861, Train Loss: 0.1175
Epoch: 861, Validation Loss: 0.4375
Total nodes: 36507
Epoch: 862, Train Loss: 0.0950
Epoch: 862, Validation Loss: 0.4296
Total nodes: 36507
Epoch: 863, Train Loss: 0.1154
Epoch: 863, Validation Loss: 0.4352
Total nodes: 36507
Epoch: 864, Train Loss: 0.1028
Epoch: 864, Validation Loss: 0.4343
Total nodes: 36507
Epoch: 865, Train Loss: 0.1156
Epoch: 865, Validation Loss: 0.4406
Total nodes: 36507
Epoch: 866, Train Loss: 0.1192
Epoch: 866, Validation Loss: 0.4289
Total nodes: 36507
Epoch: 867, Train Loss: 0.1050
Epoch: 867, Validation Loss: 0.4246
Total nodes: 36507
Epoch: 868, Train Loss: 0.1110
Epoch: 868, Validation Loss: 0.4285
Total nodes: 36507
Epoch: 869, Train Loss: 0.1216
Epoch: 869, Validation Loss: 0.4427
Total nodes: 36507
Epoch: 870, Train Loss: 0.1085
Epoch: 870, Validation Loss: 0.4386
Total nodes: 36507
Epoch: 871, Train Loss: 0.1077
Epoch: 871, Validation Loss: 0.4337
Total nodes: 36507
Epoch: 872, Train Loss: 0.1111
Epoch: 872, Validation Loss: 0.4337
Total nodes: 36507
Epoch: 873, Train Loss: 0.1096
Epoch: 873, Validation Loss: 0.4265
Total nodes: 36507
Epoch: 874, Train Loss: 0.0957
Epoch: 874, Validation Loss: 0.4317
Total nodes: 36507
Epoch: 875, Train Loss: 0.1322
Epoch: 875, Validation Loss: 0.4337
Total nodes: 36507
Epoch: 876, Train Loss: 0.1080
Epoch: 876, Validation Loss: 0.4316
Total nodes: 36507
Epoch: 877, Train Loss: 0.0879
Epoch: 877, Validation Loss: 0.4277
Total nodes: 36507
Epoch: 878, Train Loss: 0.1173
Epoch: 878, Validation Loss: 0.4295
Total nodes: 36507
Epoch: 879, Train Loss: 0.0884
Epoch: 879, Validation Loss: 0.4289
Total nodes: 36507
Epoch: 880, Train Loss: 0.1114
Epoch: 880, Validation Loss: 0.4354
Total nodes: 36507
Epoch: 881, Train Loss: 0.0944
Epoch: 881, Validation Loss: 0.4318
Total nodes: 36507
Epoch: 882, Train Loss: 0.1112
Epoch: 882, Validation Loss: 0.4319
Total nodes: 36507
Epoch: 883, Train Loss: 0.0965
Epoch: 883, Validation Loss: 0.4305
Total nodes: 36507
Epoch: 884, Train Loss: 0.0999
Epoch: 884, Validation Loss: 0.4311
Total nodes: 36507
Epoch: 885, Train Loss: 0.0978
Epoch: 885, Validation Loss: 0.4353
Total nodes: 36507
Epoch: 886, Train Loss: 0.1093
Epoch: 886, Validation Loss: 0.4299
Total nodes: 36507
Epoch: 887, Train Loss: 0.0933
Epoch: 887, Validation Loss: 0.4304
Total nodes: 36507
Epoch: 888, Train Loss: 0.1285
Epoch: 888, Validation Loss: 0.4303
Total nodes: 36507
Epoch: 889, Train Loss: 0.1114
Epoch: 889, Validation Loss: 0.4340
Total nodes: 36507
Epoch: 890, Train Loss: 0.1023
Epoch: 890, Validation Loss: 0.4350
Total nodes: 36507
Epoch: 891, Train Loss: 0.1058
Epoch: 891, Validation Loss: 0.4338
Total nodes: 36507
Epoch: 892, Train Loss: 0.0934
Epoch: 892, Validation Loss: 0.4304
Total nodes: 36507
Epoch: 893, Train Loss: 0.1021
Epoch: 893, Validation Loss: 0.4319
Total nodes: 36507
Epoch: 894, Train Loss: 0.1045
Epoch: 894, Validation Loss: 0.4277
Total nodes: 36507
Epoch: 895, Train Loss: 0.1073
Epoch: 895, Validation Loss: 0.4328
Total nodes: 36507
Epoch: 896, Train Loss: 0.1174
Epoch: 896, Validation Loss: 0.4386
Total nodes: 36507
Epoch: 897, Train Loss: 0.1055
Epoch: 897, Validation Loss: 0.4379
Total nodes: 36507
Epoch: 898, Train Loss: 0.1164
Epoch: 898, Validation Loss: 0.4299
Total nodes: 36507
Epoch: 899, Train Loss: 0.1044
Epoch: 899, Validation Loss: 0.4361
Total nodes: 36507
Epoch: 900, Train Loss: 0.1012
Epoch: 900, Validation Loss: 0.4355
Total nodes: 36507
Epoch: 901, Train Loss: 0.1254
Epoch: 901, Validation Loss: 0.4366
Total nodes: 36507
Epoch: 902, Train Loss: 0.1251
Epoch: 902, Validation Loss: 0.4297
Total nodes: 36507
Epoch: 903, Train Loss: 0.1044
Epoch: 903, Validation Loss: 0.4313
Total nodes: 36507
Epoch: 904, Train Loss: 0.1013
Epoch: 904, Validation Loss: 0.4317
Total nodes: 36507
Epoch: 905, Train Loss: 0.1047
Epoch: 905, Validation Loss: 0.4328
Total nodes: 36507
Epoch: 906, Train Loss: 0.1022
Epoch: 906, Validation Loss: 0.4343
Total nodes: 36507
Epoch: 907, Train Loss: 0.1093
Epoch: 907, Validation Loss: 0.4350
Total nodes: 36507
Epoch: 908, Train Loss: 0.1312
Epoch: 908, Validation Loss: 0.4395
Total nodes: 36507
Epoch: 909, Train Loss: 0.1105
Epoch: 909, Validation Loss: 0.4347
Total nodes: 36507
Epoch: 910, Train Loss: 0.1126
Epoch: 910, Validation Loss: 0.4330
Total nodes: 36507
Epoch: 911, Train Loss: 0.0988
Epoch: 911, Validation Loss: 0.4294
Total nodes: 36507
Epoch: 912, Train Loss: 0.1005
Epoch: 912, Validation Loss: 0.4311
Total nodes: 36507
Epoch: 913, Train Loss: 0.1048
Epoch: 913, Validation Loss: 0.4355
Total nodes: 36507
Epoch: 914, Train Loss: 0.1457
Epoch: 914, Validation Loss: 0.4421
Total nodes: 36507
Epoch: 915, Train Loss: 0.1151
Epoch: 915, Validation Loss: 0.4366
Total nodes: 36507
Epoch: 916, Train Loss: 0.0994
Epoch: 916, Validation Loss: 0.4244
Total nodes: 36507
Epoch: 917, Train Loss: 0.1159
Epoch: 917, Validation Loss: 0.4314
Total nodes: 36507
Epoch: 918, Train Loss: 0.0931
Epoch: 918, Validation Loss: 0.4301
Total nodes: 36507
Epoch: 919, Train Loss: 0.0971
Epoch: 919, Validation Loss: 0.4303
Total nodes: 36507
Epoch: 920, Train Loss: 0.1078
Epoch: 920, Validation Loss: 0.4395
Total nodes: 36507
Epoch: 921, Train Loss: 0.1309
Epoch: 921, Validation Loss: 0.4325
Total nodes: 36507
Epoch: 922, Train Loss: 0.1068
Epoch: 922, Validation Loss: 0.4240
Total nodes: 36507
Epoch: 923, Train Loss: 0.1111
Epoch: 923, Validation Loss: 0.4282
Total nodes: 36507
Epoch: 924, Train Loss: 0.1105
Epoch: 924, Validation Loss: 0.4325
Total nodes: 36507
Epoch: 925, Train Loss: 0.1240
Epoch: 925, Validation Loss: 0.4380
Total nodes: 36507
Epoch: 926, Train Loss: 0.1086
Epoch: 926, Validation Loss: 0.4387
Total nodes: 36507
Epoch: 927, Train Loss: 0.1039
Epoch: 927, Validation Loss: 0.4409
Total nodes: 36507
Epoch: 928, Train Loss: 0.0982
Epoch: 928, Validation Loss: 0.4330
Total nodes: 36507
Epoch: 929, Train Loss: 0.0995
Epoch: 929, Validation Loss: 0.4285
Total nodes: 36507
Epoch: 930, Train Loss: 0.1075
Epoch: 930, Validation Loss: 0.4320
Total nodes: 36507
Epoch: 931, Train Loss: 0.1094
Epoch: 931, Validation Loss: 0.4298
Total nodes: 36507
Epoch: 932, Train Loss: 0.1256
Epoch: 932, Validation Loss: 0.4434
Total nodes: 36507
Epoch: 933, Train Loss: 0.1093
Epoch: 933, Validation Loss: 0.4352
Total nodes: 36507
Epoch: 934, Train Loss: 0.1047
Epoch: 934, Validation Loss: 0.4374
Total nodes: 36507
Epoch: 935, Train Loss: 0.0969
Epoch: 935, Validation Loss: 0.4313
Total nodes: 36507
Epoch: 936, Train Loss: 0.1233
Epoch: 936, Validation Loss: 0.4385
Total nodes: 36507
Epoch: 937, Train Loss: 0.1192
Epoch: 937, Validation Loss: 0.4505
Total nodes: 36507
Epoch: 938, Train Loss: 0.1059
Epoch: 938, Validation Loss: 0.4442
Total nodes: 36507
Epoch: 939, Train Loss: 0.0907
Epoch: 939, Validation Loss: 0.4395
Total nodes: 36507
Epoch: 940, Train Loss: 0.1131
Epoch: 940, Validation Loss: 0.4380
Total nodes: 36507
Epoch: 941, Train Loss: 0.1084
Epoch: 941, Validation Loss: 0.4362
Total nodes: 36507
Epoch: 942, Train Loss: 0.1131
Epoch: 942, Validation Loss: 0.4313
Total nodes: 36507
Epoch: 943, Train Loss: 0.1234
Epoch: 943, Validation Loss: 0.4332
Total nodes: 36507
Epoch: 944, Train Loss: 0.1001
Epoch: 944, Validation Loss: 0.4341
Total nodes: 36507
Epoch: 945, Train Loss: 0.1192
Epoch: 945, Validation Loss: 0.4275
Total nodes: 36507
Epoch: 946, Train Loss: 0.1062
Epoch: 946, Validation Loss: 0.4401
Total nodes: 36507
Epoch: 947, Train Loss: 0.1019
Epoch: 947, Validation Loss: 0.4324
Total nodes: 36507
Epoch: 948, Train Loss: 0.1085
Epoch: 948, Validation Loss: 0.4332
Total nodes: 36507
Epoch: 949, Train Loss: 0.1017
Epoch: 949, Validation Loss: 0.4347
Total nodes: 36507
Epoch: 950, Train Loss: 0.1099
Epoch: 950, Validation Loss: 0.4393
Total nodes: 36507
Epoch: 951, Train Loss: 0.0986
Epoch: 951, Validation Loss: 0.4358
Total nodes: 36507
Epoch: 952, Train Loss: 0.1141
Epoch: 952, Validation Loss: 0.4317
Total nodes: 36507
Epoch: 953, Train Loss: 0.1178
Epoch: 953, Validation Loss: 0.4383
Total nodes: 36507
Epoch: 954, Train Loss: 0.1178
Epoch: 954, Validation Loss: 0.4336
Total nodes: 36507
Epoch: 955, Train Loss: 0.0968
Epoch: 955, Validation Loss: 0.4413
Total nodes: 36507
Epoch: 956, Train Loss: 0.0973
Epoch: 956, Validation Loss: 0.4352
Total nodes: 36507
Epoch: 957, Train Loss: 0.0962
Epoch: 957, Validation Loss: 0.4318
Total nodes: 36507
Epoch: 958, Train Loss: 0.1202
Epoch: 958, Validation Loss: 0.4397
Total nodes: 36507
Epoch: 959, Train Loss: 0.1090
Epoch: 959, Validation Loss: 0.4315
Total nodes: 36507
Epoch: 960, Train Loss: 0.1100
Epoch: 960, Validation Loss: 0.4280
Total nodes: 36507
Epoch: 961, Train Loss: 0.1063
Epoch: 961, Validation Loss: 0.4289
Total nodes: 36507
Epoch: 962, Train Loss: 0.1442
Epoch: 962, Validation Loss: 0.4505
Total nodes: 36507
Epoch: 963, Train Loss: 0.1244
Epoch: 963, Validation Loss: 0.4402
Total nodes: 36507
Epoch: 964, Train Loss: 0.0950
Epoch: 964, Validation Loss: 0.4315
Total nodes: 36507
Epoch: 965, Train Loss: 0.0956
Epoch: 965, Validation Loss: 0.4263
Total nodes: 36507
Epoch: 966, Train Loss: 0.1020
Epoch: 966, Validation Loss: 0.4325
Total nodes: 36507
Epoch: 967, Train Loss: 0.1206
Epoch: 967, Validation Loss: 0.4394
Total nodes: 36507
Epoch: 968, Train Loss: 0.0954
Epoch: 968, Validation Loss: 0.4252
Total nodes: 36507
Epoch: 969, Train Loss: 0.1097
Epoch: 969, Validation Loss: 0.4389
Total nodes: 36507
Epoch: 970, Train Loss: 0.1131
Epoch: 970, Validation Loss: 0.4345
Total nodes: 36507
Epoch: 971, Train Loss: 0.1112
Epoch: 971, Validation Loss: 0.4398
Total nodes: 36507
Epoch: 972, Train Loss: 0.0999
Epoch: 972, Validation Loss: 0.4350
Total nodes: 36507
Epoch: 973, Train Loss: 0.1003
Epoch: 973, Validation Loss: 0.4315
Total nodes: 36507
Epoch: 974, Train Loss: 0.1037
Epoch: 974, Validation Loss: 0.4358
Total nodes: 36507
Epoch: 975, Train Loss: 0.0996
Epoch: 975, Validation Loss: 0.4365
Total nodes: 36507
Epoch: 976, Train Loss: 0.1122
Epoch: 976, Validation Loss: 0.4338
Total nodes: 36507
Early stopping for train loss!
The best model: 877th epoch
Reading graph:   0%|          | 0/14983997 [00:00<?, ?it/s]Reading graph:   2%|▏         | 228974/14983997 [00:00<00:06, 2289572.96it/s]Reading graph:   3%|▎         | 484166/14983997 [00:00<00:05, 2443850.36it/s]Reading graph:   5%|▌         | 751851/14983997 [00:00<00:06, 2368587.96it/s]Reading graph:   7%|▋         | 989298/14983997 [00:00<00:05, 2341426.03it/s]Reading graph:   8%|▊         | 1223728/14983997 [00:00<00:06, 2290451.62it/s]Reading graph:  10%|▉         | 1463283/14983997 [00:00<00:05, 2324756.40it/s]Reading graph:  11%|█▏        | 1714769/14983997 [00:00<00:05, 2268452.31it/s]Reading graph:  13%|█▎        | 1942193/14983997 [00:00<00:05, 2206790.00it/s]Reading graph:  14%|█▍        | 2170259/14983997 [00:00<00:05, 2206842.93it/s]Reading graph:  16%|█▋        | 2443878/14983997 [00:01<00:05, 2362811.95it/s]Reading graph:  18%|█▊        | 2722140/14983997 [00:01<00:04, 2487156.42it/s]Reading graph:  20%|█▉        | 2984207/14983997 [00:01<00:04, 2526833.08it/s]Reading graph:  22%|██▏       | 3262088/14983997 [00:01<00:04, 2601933.33it/s]Reading graph:  24%|██▎       | 3539900/14983997 [00:01<00:04, 2654540.49it/s]Reading graph:  25%|██▌       | 3810376/14983997 [00:01<00:04, 2669528.95it/s]Reading graph:  27%|██▋       | 4083142/14983997 [00:01<00:04, 2686904.91it/s]Reading graph:  29%|██▉       | 4356912/14983997 [00:01<00:03, 2702096.20it/s]Reading graph:  31%|███       | 4634440/14983997 [00:01<00:03, 2724001.60it/s]Reading graph:  33%|███▎      | 4906991/14983997 [00:01<00:03, 2717989.58it/s]Reading graph:  35%|███▍      | 5178898/14983997 [00:02<00:03, 2678982.64it/s]Reading graph:  36%|███▋      | 5446988/14983997 [00:02<00:03, 2678595.68it/s]Reading graph:  38%|███▊      | 5715545/14983997 [00:02<00:03, 2680644.41it/s]Reading graph:  40%|███▉      | 5985147/14983997 [00:02<00:03, 2685201.79it/s]Reading graph:  42%|████▏     | 6259283/14983997 [00:02<00:03, 2701948.86it/s]Reading graph:  44%|████▎     | 6529534/14983997 [00:02<00:03, 2239451.58it/s]Reading graph:  45%|████▌     | 6796657/14983997 [00:02<00:03, 2352356.71it/s]Reading graph:  47%|████▋     | 7065842/14983997 [00:02<00:03, 2444674.14it/s]Reading graph:  49%|████▉     | 7335982/14983997 [00:02<00:03, 2516490.05it/s]Reading graph:  51%|█████     | 7609062/14983997 [00:03<00:02, 2577670.12it/s]Reading graph:  53%|█████▎    | 7884299/14983997 [00:03<00:02, 2628264.79it/s]Reading graph:  54%|█████▍    | 8154918/14983997 [00:03<00:02, 2651054.57it/s]Reading graph:  56%|█████▋    | 8429568/14983997 [00:03<00:02, 2679124.06it/s]Reading graph:  58%|█████▊    | 8707998/14983997 [00:03<00:02, 2710266.32it/s]Reading graph:  60%|█████▉    | 8982287/14983997 [00:03<00:02, 2719946.51it/s]Reading graph:  62%|██████▏   | 9255222/14983997 [00:03<00:02, 2698328.44it/s]Reading graph:  64%|██████▎   | 9537301/14983997 [00:03<00:01, 2734655.14it/s]Reading graph:  66%|██████▌   | 9849143/14983997 [00:03<00:01, 2848906.28it/s]Reading graph:  68%|██████▊   | 10152960/14983997 [00:03<00:01, 2905323.19it/s]Reading graph:  70%|██████▉   | 10457526/14983997 [00:04<00:01, 2947242.79it/s]Reading graph:  72%|███████▏  | 10765800/14983997 [00:04<00:01, 2987752.85it/s]Reading graph:  74%|███████▍  | 11073687/14983997 [00:04<00:01, 3015005.33it/s]Reading graph:  76%|███████▌  | 11381535/14983997 [00:04<00:01, 3033989.83it/s]Reading graph:  78%|███████▊  | 11691329/14983997 [00:04<00:01, 3053121.51it/s]Reading graph:  80%|████████  | 12001507/14983997 [00:04<00:00, 3067671.21it/s]Reading graph:  82%|████████▏ | 12308325/14983997 [00:04<00:00, 3013521.41it/s]Reading graph:  84%|████████▍ | 12611864/14983997 [00:04<00:00, 3019947.88it/s]Reading graph:  86%|████████▌ | 12915024/14983997 [00:04<00:00, 3023372.24it/s]Reading graph:  88%|████████▊ | 13217492/14983997 [00:04<00:00, 3021915.85it/s]Reading graph:  90%|█████████ | 13520874/14983997 [00:05<00:00, 3025425.39it/s]Reading graph:  92%|█████████▏| 13823482/14983997 [00:05<00:00, 2998952.10it/s]Reading graph:  94%|█████████▍| 14123468/14983997 [00:05<00:00, 2932076.93it/s]Reading graph:  96%|█████████▌| 14417020/14983997 [00:05<00:00, 2897110.67it/s]Reading graph:  98%|█████████▊| 14706997/14983997 [00:05<00:00, 2868071.53it/s]                                                                               