Reading graph:   0%|          | 0/7955779 [00:00<?, ?it/s]Reading graph:   2%|▏         | 195500/7955779 [00:00<00:03, 1954806.12it/s]Reading graph:   5%|▌         | 413706/7955779 [00:00<00:03, 2088372.22it/s]Reading graph:   8%|▊         | 622544/7955779 [00:00<00:03, 1983326.66it/s]Reading graph:  10%|█         | 826299/7955779 [00:00<00:03, 2004039.38it/s]Reading graph:  13%|█▎        | 1027129/7955779 [00:00<00:03, 1994100.03it/s]Reading graph:  16%|█▌        | 1252902/7955779 [00:00<00:03, 2082172.80it/s]Reading graph:  19%|█▊        | 1485136/7955779 [00:00<00:02, 2159747.91it/s]Reading graph:  22%|██▏       | 1711039/7955779 [00:00<00:02, 2191074.56it/s]Reading graph:  24%|██▍       | 1940058/7955779 [00:00<00:02, 2221862.36it/s]Reading graph:  27%|██▋       | 2170649/7955779 [00:01<00:02, 2247681.88it/s]Reading graph:  30%|███       | 2403363/7955779 [00:01<00:02, 2271904.93it/s]Reading graph:  33%|███▎      | 2636211/7955779 [00:01<00:02, 2289042.96it/s]Reading graph:  36%|███▌      | 2871694/7955779 [00:01<00:02, 2308916.80it/s]Reading graph:  39%|███▉      | 3107315/7955779 [00:01<00:02, 2323155.72it/s]Reading graph:  42%|████▏     | 3342233/7955779 [00:01<00:01, 2330965.66it/s]Reading graph:  45%|████▍     | 3577455/7955779 [00:01<00:01, 2337334.09it/s]Reading graph:  48%|████▊     | 3814094/7955779 [00:01<00:01, 2346035.32it/s]Reading graph:  51%|█████     | 4050038/7955779 [00:01<00:01, 2350032.68it/s]Reading graph:  54%|█████▍    | 4321305/7955779 [00:01<00:01, 2458897.54it/s]Reading graph:  58%|█████▊    | 4608687/7955779 [00:02<00:01, 2583426.28it/s]Reading graph:  62%|██████▏   | 4903680/7955779 [00:02<00:01, 2693387.90it/s]Reading graph:  65%|██████▌   | 5198117/7955779 [00:02<00:00, 2768672.94it/s]Reading graph:  69%|██████▉   | 5496431/7955779 [00:02<00:00, 2832990.78it/s]Reading graph:  73%|███████▎  | 5795544/7955779 [00:02<00:00, 2880406.90it/s]Reading graph:  77%|███████▋  | 6094906/7955779 [00:02<00:00, 2914339.21it/s]Reading graph:  80%|████████  | 6396100/7955779 [00:02<00:00, 2943583.95it/s]Reading graph:  84%|████████▍ | 6695402/7955779 [00:02<00:00, 2958371.60it/s]Reading graph:  88%|████████▊ | 6999522/7955779 [00:02<00:00, 2983189.20it/s]Reading graph:  92%|█████████▏| 7304262/7955779 [00:02<00:00, 3002416.78it/s]Reading graph:  96%|█████████▌| 7608832/7955779 [00:03<00:00, 3015359.36it/s]Reading graph:  99%|█████████▉| 7913230/7955779 [00:03<00:00, 3023900.86it/s]                                                                             Reading graph:   0%|          | 0/2286395 [00:00<?, ?it/s]Reading graph:   9%|▉         | 207089/2286395 [00:00<00:01, 2070773.48it/s]Reading graph:  19%|█▉        | 439455/2286395 [00:00<00:00, 2219481.37it/s]Reading graph:  29%|██▉       | 668309/2286395 [00:00<00:00, 2192001.79it/s]Reading graph:  39%|███▉      | 887593/2286395 [00:00<00:00, 2147268.57it/s]Reading graph:  48%|████▊     | 1102440/2286395 [00:00<00:00, 2058893.32it/s]Reading graph:  59%|█████▉    | 1354875/2286395 [00:00<00:00, 2108909.57it/s]Reading graph:  72%|███████▏  | 1640463/2286395 [00:00<00:00, 2338366.77it/s]Reading graph:  84%|████████▍ | 1923761/2286395 [00:00<00:00, 2489203.00it/s]Reading graph:  95%|█████████▌| 2174391/2286395 [00:00<00:00, 2469296.11it/s]                                                                             Computing METIS partitioning...
Done!
Computing METIS partitioning...
Done!
Total nodes: 29210
Epoch: 001, Train Loss: 0.6623, Validation Loss: 0.6835
Total nodes: 29210
Epoch: 002, Train Loss: 0.5737, Validation Loss: 0.6647
Total nodes: 29210
Epoch: 003, Train Loss: 0.4458, Validation Loss: 0.6480
Total nodes: 29210
Epoch: 004, Train Loss: 0.3601, Validation Loss: 0.6457
Total nodes: 29210
Epoch: 005, Train Loss: 0.3474, Validation Loss: 0.6437
Total nodes: 29210
Epoch: 006, Train Loss: 0.3414, Validation Loss: 0.6408
Total nodes: 29210
Epoch: 007, Train Loss: 0.3371, Validation Loss: 0.6297
Total nodes: 29210
Epoch: 008, Train Loss: 0.3346, Validation Loss: 0.6267
Total nodes: 29210
Epoch: 009, Train Loss: 0.3241, Validation Loss: 0.6170
Total nodes: 29210
Epoch: 010, Train Loss: 0.3115, Validation Loss: 0.6101
Total nodes: 29210
Epoch: 011, Train Loss: 0.3067, Validation Loss: 0.6009
Total nodes: 29210
Epoch: 012, Train Loss: 0.2998, Validation Loss: 0.5962
Total nodes: 29210
Epoch: 013, Train Loss: 0.2970, Validation Loss: 0.5863
Total nodes: 29210
Epoch: 014, Train Loss: 0.2857, Validation Loss: 0.5755
Total nodes: 29210
Epoch: 015, Train Loss: 0.2797, Validation Loss: 0.5706
Total nodes: 29210
Epoch: 016, Train Loss: 0.2734, Validation Loss: 0.5598
Total nodes: 29210
Epoch: 017, Train Loss: 0.2667, Validation Loss: 0.5485
Total nodes: 29210
Epoch: 018, Train Loss: 0.2601, Validation Loss: 0.5410
Total nodes: 29210
Epoch: 019, Train Loss: 0.2531, Validation Loss: 0.5345
Total nodes: 29210
Epoch: 020, Train Loss: 0.2434, Validation Loss: 0.5268
Total nodes: 29210
Epoch: 021, Train Loss: 0.2379, Validation Loss: 0.5248
Total nodes: 29210
Epoch: 022, Train Loss: 0.2316, Validation Loss: 0.5272
Total nodes: 29210
Epoch: 023, Train Loss: 0.2243, Validation Loss: 0.5353
Total nodes: 29210
Epoch: 024, Train Loss: 0.2159, Validation Loss: 0.5273
Total nodes: 29210
Epoch: 025, Train Loss: 0.2104, Validation Loss: 0.5355
Total nodes: 29210
Epoch: 026, Train Loss: 0.2051, Validation Loss: 0.5424
Total nodes: 29210
Epoch: 027, Train Loss: 0.1987, Validation Loss: 0.5510
Total nodes: 29210
Epoch: 028, Train Loss: 0.1988, Validation Loss: 0.5542
Total nodes: 29210
Epoch: 029, Train Loss: 0.1913, Validation Loss: 0.5472
Total nodes: 29210
Epoch: 030, Train Loss: 0.1880, Validation Loss: 0.5564
Total nodes: 29210
Epoch: 031, Train Loss: 0.1837, Validation Loss: 0.5565
Total nodes: 29210
Change the learning ratio for validation loss!
Epoch: 032, Train Loss: 0.1802, Validation Loss: 0.5729
Total nodes: 29210
Epoch: 033, Train Loss: 0.1799, Validation Loss: 0.5691
Total nodes: 29210
Epoch: 034, Train Loss: 0.1757, Validation Loss: 0.5685
Total nodes: 29210
Epoch: 035, Train Loss: 0.1786, Validation Loss: 0.5733
Total nodes: 29210
Epoch: 036, Train Loss: 0.1750, Validation Loss: 0.5724
Total nodes: 29210
Epoch: 037, Train Loss: 0.1759, Validation Loss: 0.5734
Total nodes: 29210
Epoch: 038, Train Loss: 0.1761, Validation Loss: 0.5719
Total nodes: 29210
Epoch: 039, Train Loss: 0.1756, Validation Loss: 0.5762
Total nodes: 29210
Epoch: 040, Train Loss: 0.1751, Validation Loss: 0.5717
Total nodes: 29210
Epoch: 041, Train Loss: 0.1757, Validation Loss: 0.5818
Total nodes: 29210
Epoch: 042, Train Loss: 0.1754, Validation Loss: 0.5764
Total nodes: 29210
Epoch: 043, Train Loss: 0.1748, Validation Loss: 0.5751
Total nodes: 29210
Epoch: 044, Train Loss: 0.1745, Validation Loss: 0.5776
Total nodes: 29210
Epoch: 045, Train Loss: 0.1719, Validation Loss: 0.5769
Total nodes: 29210
Epoch: 046, Train Loss: 0.1730, Validation Loss: 0.5858
Total nodes: 29210
Epoch: 047, Train Loss: 0.1752, Validation Loss: 0.5816
Total nodes: 29210
Epoch: 048, Train Loss: 0.1742, Validation Loss: 0.5800
Total nodes: 29210
Epoch: 049, Train Loss: 0.1724, Validation Loss: 0.5837
Total nodes: 29210
Epoch: 050, Train Loss: 0.1748, Validation Loss: 0.5811
Total nodes: 29210
Epoch: 051, Train Loss: 0.1725, Validation Loss: 0.5752
Total nodes: 29210
Epoch: 052, Train Loss: 0.1725, Validation Loss: 0.5803
Total nodes: 29210
Epoch: 053, Train Loss: 0.1749, Validation Loss: 0.5731
Total nodes: 29210
Epoch: 054, Train Loss: 0.1734, Validation Loss: 0.5782
Total nodes: 29210
Epoch: 055, Train Loss: 0.1746, Validation Loss: 0.5686
Total nodes: 29210
Epoch: 056, Train Loss: 0.1691, Validation Loss: 0.5802
Total nodes: 29210
Epoch: 057, Train Loss: 0.1709, Validation Loss: 0.5720
Total nodes: 29210
Epoch: 058, Train Loss: 0.1711, Validation Loss: 0.5824
Total nodes: 29210
Epoch: 059, Train Loss: 0.1719, Validation Loss: 0.5835
Total nodes: 29210
Epoch: 060, Train Loss: 0.1712, Validation Loss: 0.5689
Total nodes: 29210
Epoch: 061, Train Loss: 0.1704, Validation Loss: 0.5755
Total nodes: 29210
Epoch: 062, Train Loss: 0.1692, Validation Loss: 0.5706
Total nodes: 29210
Epoch: 063, Train Loss: 0.1685, Validation Loss: 0.5750
Total nodes: 29210
Epoch: 064, Train Loss: 0.1691, Validation Loss: 0.5795
Total nodes: 29210
Epoch: 065, Train Loss: 0.1689, Validation Loss: 0.5690
Total nodes: 29210
Epoch: 066, Train Loss: 0.1703, Validation Loss: 0.5847
Total nodes: 29210
Epoch: 067, Train Loss: 0.1688, Validation Loss: 0.5692
Total nodes: 29210
Epoch: 068, Train Loss: 0.1685, Validation Loss: 0.5720
Total nodes: 29210
Epoch: 069, Train Loss: 0.1709, Validation Loss: 0.5730
Total nodes: 29210
Epoch: 070, Train Loss: 0.1706, Validation Loss: 0.5717
Total nodes: 29210
Epoch: 071, Train Loss: 0.1675, Validation Loss: 0.5767
Total nodes: 29210
Epoch: 072, Train Loss: 0.1674, Validation Loss: 0.5792
Total nodes: 29210
Epoch: 073, Train Loss: 0.1695, Validation Loss: 0.5798
Total nodes: 29210
Epoch: 074, Train Loss: 0.1681, Validation Loss: 0.5741
Total nodes: 29210
Epoch: 075, Train Loss: 0.1684, Validation Loss: 0.5611
Total nodes: 29210
Epoch: 076, Train Loss: 0.1678, Validation Loss: 0.5716
Total nodes: 29210
Epoch: 077, Train Loss: 0.1672, Validation Loss: 0.5630
Total nodes: 29210
Epoch: 078, Train Loss: 0.1677, Validation Loss: 0.5735
Total nodes: 29210
Epoch: 079, Train Loss: 0.1633, Validation Loss: 0.5673
Total nodes: 29210
Epoch: 080, Train Loss: 0.1638, Validation Loss: 0.5566
Total nodes: 29210
Epoch: 081, Train Loss: 0.1675, Validation Loss: 0.5686
Total nodes: 29210
Epoch: 082, Train Loss: 0.1643, Validation Loss: 0.5735
Total nodes: 29210
Epoch: 083, Train Loss: 0.1665, Validation Loss: 0.5607
Total nodes: 29210
Epoch: 084, Train Loss: 0.1635, Validation Loss: 0.5678
Total nodes: 29210
Epoch: 085, Train Loss: 0.1667, Validation Loss: 0.5553
Total nodes: 29210
Epoch: 086, Train Loss: 0.1649, Validation Loss: 0.5642
Total nodes: 29210
Epoch: 087, Train Loss: 0.1658, Validation Loss: 0.5610
Total nodes: 29210
Epoch: 088, Train Loss: 0.1646, Validation Loss: 0.5569
Total nodes: 29210
Epoch: 089, Train Loss: 0.1629, Validation Loss: 0.5569
Total nodes: 29210
Epoch: 090, Train Loss: 0.1653, Validation Loss: 0.5613
Total nodes: 29210
Epoch: 091, Train Loss: 0.1636, Validation Loss: 0.5530
Total nodes: 29210
Epoch: 092, Train Loss: 0.1637, Validation Loss: 0.5525
Total nodes: 29210
Epoch: 093, Train Loss: 0.1644, Validation Loss: 0.5519
Total nodes: 29210
Epoch: 094, Train Loss: 0.1607, Validation Loss: 0.5565
Total nodes: 29210
Epoch: 095, Train Loss: 0.1613, Validation Loss: 0.5642
Total nodes: 29210
Epoch: 096, Train Loss: 0.1623, Validation Loss: 0.5493
Total nodes: 29210
Epoch: 097, Train Loss: 0.1628, Validation Loss: 0.5441
Total nodes: 29210
Epoch: 098, Train Loss: 0.1601, Validation Loss: 0.5433
Total nodes: 29210
Epoch: 099, Train Loss: 0.1653, Validation Loss: 0.5474
Total nodes: 29210
Epoch: 100, Train Loss: 0.1603, Validation Loss: 0.5471
Total nodes: 29210
Epoch: 101, Train Loss: 0.1608, Validation Loss: 0.5339
Total nodes: 29210
Epoch: 102, Train Loss: 0.1615, Validation Loss: 0.5318
Total nodes: 29210
Epoch: 103, Train Loss: 0.1643, Validation Loss: 0.5322
Total nodes: 29210
Epoch: 104, Train Loss: 0.1620, Validation Loss: 0.5340
Total nodes: 29210
Epoch: 105, Train Loss: 0.1603, Validation Loss: 0.5337
Total nodes: 29210
Epoch: 106, Train Loss: 0.1609, Validation Loss: 0.5219
Total nodes: 29210
Epoch: 107, Train Loss: 0.1610, Validation Loss: 0.5338
Total nodes: 29210
Epoch: 108, Train Loss: 0.1620, Validation Loss: 0.5304
Total nodes: 29210
Epoch: 109, Train Loss: 0.1631, Validation Loss: 0.5341
Total nodes: 29210
Epoch: 110, Train Loss: 0.1623, Validation Loss: 0.5181
Total nodes: 29210
Epoch: 111, Train Loss: 0.1626, Validation Loss: 0.5169
Total nodes: 29210
Epoch: 112, Train Loss: 0.1599, Validation Loss: 0.5202
Total nodes: 29210
Epoch: 113, Train Loss: 0.1609, Validation Loss: 0.5232
Total nodes: 29210
Epoch: 114, Train Loss: 0.1603, Validation Loss: 0.5295
Total nodes: 29210
Epoch: 115, Train Loss: 0.1590, Validation Loss: 0.5186
Total nodes: 29210
Epoch: 116, Train Loss: 0.1591, Validation Loss: 0.5183
Total nodes: 29210
Epoch: 117, Train Loss: 0.1570, Validation Loss: 0.5090
Total nodes: 29210
Epoch: 118, Train Loss: 0.1558, Validation Loss: 0.5132
Total nodes: 29210
Epoch: 119, Train Loss: 0.1539, Validation Loss: 0.5032
Total nodes: 29210
Epoch: 120, Train Loss: 0.1555, Validation Loss: 0.4954
Total nodes: 29210
Epoch: 121, Train Loss: 0.1556, Validation Loss: 0.5030
Total nodes: 29210
Epoch: 122, Train Loss: 0.1576, Validation Loss: 0.5021
Total nodes: 29210
Epoch: 123, Train Loss: 0.1581, Validation Loss: 0.5048
Total nodes: 29210
Epoch: 124, Train Loss: 0.1551, Validation Loss: 0.4887
Total nodes: 29210
Epoch: 125, Train Loss: 0.1599, Validation Loss: 0.4986
Total nodes: 29210
Epoch: 126, Train Loss: 0.1552, Validation Loss: 0.4985
Total nodes: 29210
Epoch: 127, Train Loss: 0.1577, Validation Loss: 0.4872
Total nodes: 29210
Epoch: 128, Train Loss: 0.1577, Validation Loss: 0.4962
Total nodes: 29210
Epoch: 129, Train Loss: 0.1566, Validation Loss: 0.4909
Total nodes: 29210
Epoch: 130, Train Loss: 0.1563, Validation Loss: 0.4796
Total nodes: 29210
Epoch: 131, Train Loss: 0.1541, Validation Loss: 0.4792
Total nodes: 29210
Epoch: 132, Train Loss: 0.1550, Validation Loss: 0.4727
Total nodes: 29210
Epoch: 133, Train Loss: 0.1563, Validation Loss: 0.4690
Total nodes: 29210
Epoch: 134, Train Loss: 0.1560, Validation Loss: 0.4670
Total nodes: 29210
Epoch: 135, Train Loss: 0.1573, Validation Loss: 0.4640
Total nodes: 29210
Epoch: 136, Train Loss: 0.1531, Validation Loss: 0.4618
Total nodes: 29210
Epoch: 137, Train Loss: 0.1528, Validation Loss: 0.4615
Total nodes: 29210
Epoch: 138, Train Loss: 0.1540, Validation Loss: 0.4538
Total nodes: 29210
Epoch: 139, Train Loss: 0.1541, Validation Loss: 0.4555
Total nodes: 29210
Epoch: 140, Train Loss: 0.1539, Validation Loss: 0.4475
Total nodes: 29210
Epoch: 141, Train Loss: 0.1532, Validation Loss: 0.4378
Total nodes: 29210
Epoch: 142, Train Loss: 0.1494, Validation Loss: 0.4458
Total nodes: 29210
Epoch: 143, Train Loss: 0.1550, Validation Loss: 0.4395
Total nodes: 29210
Epoch: 144, Train Loss: 0.1501, Validation Loss: 0.4398
Total nodes: 29210
Epoch: 145, Train Loss: 0.1510, Validation Loss: 0.4245
Total nodes: 29210
Epoch: 146, Train Loss: 0.1535, Validation Loss: 0.4207
Total nodes: 29210
Epoch: 147, Train Loss: 0.1510, Validation Loss: 0.4350
Total nodes: 29210
Epoch: 148, Train Loss: 0.1484, Validation Loss: 0.4179
Total nodes: 29210
Epoch: 149, Train Loss: 0.1506, Validation Loss: 0.4232
Total nodes: 29210
Epoch: 150, Train Loss: 0.1515, Validation Loss: 0.4214
Total nodes: 29210
Epoch: 151, Train Loss: 0.1540, Validation Loss: 0.4121
Total nodes: 29210
Epoch: 152, Train Loss: 0.1523, Validation Loss: 0.4191
Total nodes: 29210
Epoch: 153, Train Loss: 0.1499, Validation Loss: 0.4022
Total nodes: 29210
Epoch: 154, Train Loss: 0.1493, Validation Loss: 0.3983
Total nodes: 29210
Epoch: 155, Train Loss: 0.1501, Validation Loss: 0.3960
Total nodes: 29210
Epoch: 156, Train Loss: 0.1489, Validation Loss: 0.3950
Total nodes: 29210
Epoch: 157, Train Loss: 0.1488, Validation Loss: 0.3967
Total nodes: 29210
Epoch: 158, Train Loss: 0.1475, Validation Loss: 0.3921
Total nodes: 29210
Epoch: 159, Train Loss: 0.1476, Validation Loss: 0.3859
Total nodes: 29210
Epoch: 160, Train Loss: 0.1487, Validation Loss: 0.3790
Total nodes: 29210
Epoch: 161, Train Loss: 0.1470, Validation Loss: 0.3724
Total nodes: 29210
Epoch: 162, Train Loss: 0.1491, Validation Loss: 0.3773
Total nodes: 29210
Epoch: 163, Train Loss: 0.1450, Validation Loss: 0.3608
Total nodes: 29210
Epoch: 164, Train Loss: 0.1478, Validation Loss: 0.3572
Total nodes: 29210
Epoch: 165, Train Loss: 0.1473, Validation Loss: 0.3652
Total nodes: 29210
Epoch: 166, Train Loss: 0.1472, Validation Loss: 0.3537
Total nodes: 29210
Epoch: 167, Train Loss: 0.1475, Validation Loss: 0.3502
Total nodes: 29210
Epoch: 168, Train Loss: 0.1438, Validation Loss: 0.3504
Total nodes: 29210
Epoch: 169, Train Loss: 0.1470, Validation Loss: 0.3361
Total nodes: 29210
Epoch: 170, Train Loss: 0.1448, Validation Loss: 0.3404
Total nodes: 29210
Epoch: 171, Train Loss: 0.1432, Validation Loss: 0.3293
Total nodes: 29210
Epoch: 172, Train Loss: 0.1460, Validation Loss: 0.3361
Total nodes: 29210
Epoch: 173, Train Loss: 0.1433, Validation Loss: 0.3251
Total nodes: 29210
Epoch: 174, Train Loss: 0.1415, Validation Loss: 0.3271
Total nodes: 29210
Epoch: 175, Train Loss: 0.1423, Validation Loss: 0.3222
Total nodes: 29210
Epoch: 176, Train Loss: 0.1424, Validation Loss: 0.3201
Total nodes: 29210
Epoch: 177, Train Loss: 0.1397, Validation Loss: 0.3118
Total nodes: 29210
Epoch: 178, Train Loss: 0.1430, Validation Loss: 0.3130
Total nodes: 29210
Epoch: 179, Train Loss: 0.1431, Validation Loss: 0.3160
Total nodes: 29210
Epoch: 180, Train Loss: 0.1411, Validation Loss: 0.3144
Total nodes: 29210
Epoch: 181, Train Loss: 0.1438, Validation Loss: 0.3070
Total nodes: 29210
Epoch: 182, Train Loss: 0.1423, Validation Loss: 0.3104
Total nodes: 29210
Epoch: 183, Train Loss: 0.1400, Validation Loss: 0.3095
Total nodes: 29210
Epoch: 184, Train Loss: 0.1377, Validation Loss: 0.3024
Total nodes: 29210
Epoch: 185, Train Loss: 0.1388, Validation Loss: 0.3007
Total nodes: 29210
Epoch: 186, Train Loss: 0.1395, Validation Loss: 0.2952
Total nodes: 29210
Epoch: 187, Train Loss: 0.1405, Validation Loss: 0.3049
Total nodes: 29210
Epoch: 188, Train Loss: 0.1378, Validation Loss: 0.2938
Total nodes: 29210
Epoch: 189, Train Loss: 0.1390, Validation Loss: 0.2914
Total nodes: 29210
Epoch: 190, Train Loss: 0.1386, Validation Loss: 0.2933
Total nodes: 29210
Epoch: 191, Train Loss: 0.1401, Validation Loss: 0.2858
Total nodes: 29210
Epoch: 192, Train Loss: 0.1363, Validation Loss: 0.2834
Total nodes: 29210
Epoch: 193, Train Loss: 0.1379, Validation Loss: 0.2839
Total nodes: 29210
Epoch: 194, Train Loss: 0.1357, Validation Loss: 0.2837
Total nodes: 29210
Epoch: 195, Train Loss: 0.1346, Validation Loss: 0.2824
Total nodes: 29210
Epoch: 196, Train Loss: 0.1358, Validation Loss: 0.2859
Total nodes: 29210
Epoch: 197, Train Loss: 0.1331, Validation Loss: 0.2798
Total nodes: 29210
Epoch: 198, Train Loss: 0.1376, Validation Loss: 0.2839
Total nodes: 29210
Epoch: 199, Train Loss: 0.1377, Validation Loss: 0.2786
Total nodes: 29210
Epoch: 200, Train Loss: 0.1371, Validation Loss: 0.2777
Total nodes: 29210
Epoch: 201, Train Loss: 0.1343, Validation Loss: 0.2799
Total nodes: 29210
Epoch: 202, Train Loss: 0.1347, Validation Loss: 0.2740
Total nodes: 29210
Epoch: 203, Train Loss: 0.1352, Validation Loss: 0.2820
Total nodes: 29210
Epoch: 204, Train Loss: 0.1355, Validation Loss: 0.2774
Total nodes: 29210
Epoch: 205, Train Loss: 0.1344, Validation Loss: 0.2763
Total nodes: 29210
Epoch: 206, Train Loss: 0.1344, Validation Loss: 0.2815
Total nodes: 29210
Epoch: 207, Train Loss: 0.1340, Validation Loss: 0.2780
Total nodes: 29210
Epoch: 208, Train Loss: 0.1349, Validation Loss: 0.2798
Total nodes: 29210
Epoch: 209, Train Loss: 0.1334, Validation Loss: 0.2736
Total nodes: 29210
Epoch: 210, Train Loss: 0.1320, Validation Loss: 0.2771
Total nodes: 29210
Epoch: 211, Train Loss: 0.1325, Validation Loss: 0.2737
Total nodes: 29210
Epoch: 212, Train Loss: 0.1297, Validation Loss: 0.2745
Total nodes: 29210
Epoch: 213, Train Loss: 0.1331, Validation Loss: 0.2700
Total nodes: 29210
Epoch: 214, Train Loss: 0.1333, Validation Loss: 0.2777
Total nodes: 29210
Epoch: 215, Train Loss: 0.1298, Validation Loss: 0.2727
Total nodes: 29210
Epoch: 216, Train Loss: 0.1308, Validation Loss: 0.2683
Total nodes: 29210
Epoch: 217, Train Loss: 0.1319, Validation Loss: 0.2772
Total nodes: 29210
Epoch: 218, Train Loss: 0.1309, Validation Loss: 0.2701
Total nodes: 29210
Epoch: 219, Train Loss: 0.1284, Validation Loss: 0.2758
Total nodes: 29210
Epoch: 220, Train Loss: 0.1314, Validation Loss: 0.2776
Total nodes: 29210
Epoch: 221, Train Loss: 0.1322, Validation Loss: 0.2756
Total nodes: 29210
Epoch: 222, Train Loss: 0.1308, Validation Loss: 0.2735
Total nodes: 29210
Epoch: 223, Train Loss: 0.1313, Validation Loss: 0.2755
Total nodes: 29210
Epoch: 224, Train Loss: 0.1307, Validation Loss: 0.2713
Total nodes: 29210
Epoch: 225, Train Loss: 0.1307, Validation Loss: 0.2731
Total nodes: 29210
Epoch: 226, Train Loss: 0.1288, Validation Loss: 0.2747
Total nodes: 29210
Epoch: 227, Train Loss: 0.1271, Validation Loss: 0.2746
Total nodes: 29210
Epoch: 228, Train Loss: 0.1302, Validation Loss: 0.2760
Total nodes: 29210
Epoch: 229, Train Loss: 0.1288, Validation Loss: 0.2721
Total nodes: 29210
Epoch: 230, Train Loss: 0.1274, Validation Loss: 0.2751
Total nodes: 29210
Epoch: 231, Train Loss: 0.1280, Validation Loss: 0.2729
Total nodes: 29210
Epoch: 232, Train Loss: 0.1281, Validation Loss: 0.2717
Total nodes: 29210
Epoch: 233, Train Loss: 0.1257, Validation Loss: 0.2706
Total nodes: 29210
Epoch: 234, Train Loss: 0.1303, Validation Loss: 0.2696
Total nodes: 29210
Epoch: 235, Train Loss: 0.1293, Validation Loss: 0.2711
Total nodes: 29210
Epoch: 236, Train Loss: 0.1289, Validation Loss: 0.2703
Total nodes: 29210
Epoch: 237, Train Loss: 0.1264, Validation Loss: 0.2745
Total nodes: 29210
Epoch: 238, Train Loss: 0.1263, Validation Loss: 0.2782
Total nodes: 29210
Epoch: 239, Train Loss: 0.1253, Validation Loss: 0.2679
Total nodes: 29210
Epoch: 240, Train Loss: 0.1299, Validation Loss: 0.2729
Total nodes: 29210
Epoch: 241, Train Loss: 0.1264, Validation Loss: 0.2718
Total nodes: 29210
Epoch: 242, Train Loss: 0.1289, Validation Loss: 0.2768
Total nodes: 29210
Epoch: 243, Train Loss: 0.1281, Validation Loss: 0.2783
Total nodes: 29210
Epoch: 244, Train Loss: 0.1258, Validation Loss: 0.2748
Total nodes: 29210
Epoch: 245, Train Loss: 0.1240, Validation Loss: 0.2771
Total nodes: 29210
Epoch: 246, Train Loss: 0.1279, Validation Loss: 0.2686
Total nodes: 29210
Epoch: 247, Train Loss: 0.1238, Validation Loss: 0.2703
Total nodes: 29210
Epoch: 248, Train Loss: 0.1257, Validation Loss: 0.2686
Total nodes: 29210
Epoch: 249, Train Loss: 0.1260, Validation Loss: 0.2750
Total nodes: 29210
Epoch: 250, Train Loss: 0.1236, Validation Loss: 0.2660
Total nodes: 29210
Epoch: 251, Train Loss: 0.1256, Validation Loss: 0.2730
Total nodes: 29210
Epoch: 252, Train Loss: 0.1237, Validation Loss: 0.2727
Total nodes: 29210
Epoch: 253, Train Loss: 0.1226, Validation Loss: 0.2750
Total nodes: 29210
Epoch: 254, Train Loss: 0.1225, Validation Loss: 0.2772
Total nodes: 29210
Epoch: 255, Train Loss: 0.1245, Validation Loss: 0.2762
Total nodes: 29210
Epoch: 256, Train Loss: 0.1228, Validation Loss: 0.2786
Total nodes: 29210
Epoch: 257, Train Loss: 0.1242, Validation Loss: 0.2708
Total nodes: 29210
Epoch: 258, Train Loss: 0.1237, Validation Loss: 0.2702
Total nodes: 29210
Epoch: 259, Train Loss: 0.1251, Validation Loss: 0.2713
Total nodes: 29210
Epoch: 260, Train Loss: 0.1244, Validation Loss: 0.2626
Total nodes: 29210
Epoch: 261, Train Loss: 0.1220, Validation Loss: 0.2758
Total nodes: 29210
Epoch: 262, Train Loss: 0.1260, Validation Loss: 0.2705
Total nodes: 29210
Epoch: 263, Train Loss: 0.1243, Validation Loss: 0.2746
Total nodes: 29210
Epoch: 264, Train Loss: 0.1238, Validation Loss: 0.2758
Total nodes: 29210
Epoch: 265, Train Loss: 0.1223, Validation Loss: 0.2707
Total nodes: 29210
Epoch: 266, Train Loss: 0.1221, Validation Loss: 0.2679
Total nodes: 29210
Epoch: 267, Train Loss: 0.1207, Validation Loss: 0.2690
Total nodes: 29210
Epoch: 268, Train Loss: 0.1233, Validation Loss: 0.2753
Total nodes: 29210
Epoch: 269, Train Loss: 0.1210, Validation Loss: 0.2741
Total nodes: 29210
Epoch: 270, Train Loss: 0.1202, Validation Loss: 0.2745
Total nodes: 29210
Epoch: 271, Train Loss: 0.1234, Validation Loss: 0.2805
Total nodes: 29210
Epoch: 272, Train Loss: 0.1204, Validation Loss: 0.2762
Total nodes: 29210
Epoch: 273, Train Loss: 0.1201, Validation Loss: 0.2755
Total nodes: 29210
Epoch: 274, Train Loss: 0.1216, Validation Loss: 0.2675
Total nodes: 29210
Epoch: 275, Train Loss: 0.1218, Validation Loss: 0.2723
Total nodes: 29210
Epoch: 276, Train Loss: 0.1234, Validation Loss: 0.2708
Total nodes: 29210
Epoch: 277, Train Loss: 0.1193, Validation Loss: 0.2763
Total nodes: 29210
Epoch: 278, Train Loss: 0.1199, Validation Loss: 0.2703
Total nodes: 29210
Epoch: 279, Train Loss: 0.1212, Validation Loss: 0.2688
Total nodes: 29210
Epoch: 280, Train Loss: 0.1199, Validation Loss: 0.2753
Total nodes: 29210
Epoch: 281, Train Loss: 0.1210, Validation Loss: 0.2722
Total nodes: 29210
Epoch: 282, Train Loss: 0.1213, Validation Loss: 0.2752
Total nodes: 29210
Epoch: 283, Train Loss: 0.1151, Validation Loss: 0.2750
Total nodes: 29210
Epoch: 284, Train Loss: 0.1219, Validation Loss: 0.2718
Total nodes: 29210
Epoch: 285, Train Loss: 0.1212, Validation Loss: 0.2697
Total nodes: 29210
Epoch: 286, Train Loss: 0.1189, Validation Loss: 0.2724
Total nodes: 29210
Epoch: 287, Train Loss: 0.1231, Validation Loss: 0.2754
Total nodes: 29210
Epoch: 288, Train Loss: 0.1211, Validation Loss: 0.2764
Total nodes: 29210
Epoch: 289, Train Loss: 0.1225, Validation Loss: 0.2766
Total nodes: 29210
Epoch: 290, Train Loss: 0.1213, Validation Loss: 0.2763
Total nodes: 29210
Epoch: 291, Train Loss: 0.1168, Validation Loss: 0.2708
Total nodes: 29210
Epoch: 292, Train Loss: 0.1184, Validation Loss: 0.2813
Total nodes: 29210
Epoch: 293, Train Loss: 0.1182, Validation Loss: 0.2720
Total nodes: 29210
Epoch: 294, Train Loss: 0.1189, Validation Loss: 0.2787
Total nodes: 29210
Epoch: 295, Train Loss: 0.1182, Validation Loss: 0.2694
Total nodes: 29210
Epoch: 296, Train Loss: 0.1178, Validation Loss: 0.2781
Total nodes: 29210
Epoch: 297, Train Loss: 0.1156, Validation Loss: 0.2772
Total nodes: 29210
Epoch: 298, Train Loss: 0.1176, Validation Loss: 0.2715
Total nodes: 29210
Epoch: 299, Train Loss: 0.1182, Validation Loss: 0.2731
Total nodes: 29210
Epoch: 300, Train Loss: 0.1139, Validation Loss: 0.2689
The best model: 300th epoch
Reading graph:   0%|          | 0/11392560 [00:00<?, ?it/s]Reading graph:   2%|▏         | 195322/11392560 [00:00<00:05, 1953124.07it/s]Reading graph:   4%|▎         | 407957/11392560 [00:00<00:05, 2054966.01it/s]Reading graph:   5%|▌         | 616076/11392560 [00:00<00:05, 2066898.28it/s]Reading graph:   7%|▋         | 822766/11392560 [00:00<00:05, 2025067.81it/s]Reading graph:   9%|▉         | 1025399/11392560 [00:00<00:05, 1898989.33it/s]Reading graph:  11%|█         | 1216510/11392560 [00:00<00:05, 1801150.31it/s]Reading graph:  12%|█▏        | 1397943/11392560 [00:00<00:05, 1747385.26it/s]Reading graph:  14%|█▍        | 1575227/11392560 [00:00<00:05, 1754897.20it/s]Reading graph:  15%|█▌        | 1751370/11392560 [00:00<00:06, 1601714.54it/s]Reading graph:  17%|█▋        | 1929117/11392560 [00:01<00:05, 1621595.89it/s]Reading graph:  19%|█▊        | 2120824/11392560 [00:01<00:05, 1704709.50it/s]Reading graph:  20%|██        | 2293272/11392560 [00:01<00:05, 1598570.39it/s]Reading graph:  22%|██▏       | 2460944/11392560 [00:01<00:05, 1620186.80it/s]Reading graph:  23%|██▎       | 2649779/11392560 [00:01<00:05, 1696162.50it/s]Reading graph:  25%|██▍       | 2821158/11392560 [00:01<00:05, 1634887.36it/s]Reading graph:  27%|██▋       | 3021926/11392560 [00:01<00:04, 1740544.00it/s]Reading graph:  28%|██▊       | 3225815/11392560 [00:01<00:04, 1826514.60it/s]Reading graph:  30%|███       | 3428377/11392560 [00:01<00:04, 1884558.22it/s]Reading graph:  32%|███▏      | 3633425/11392560 [00:02<00:04, 1933332.72it/s]Reading graph:  34%|███▎      | 3839682/11392560 [00:02<00:03, 1971541.94it/s]Reading graph:  35%|███▌      | 4037588/11392560 [00:02<00:03, 1907389.23it/s]Reading graph:  37%|███▋      | 4249353/11392560 [00:02<00:03, 1968517.00it/s]Reading graph:  39%|███▉      | 4447056/11392560 [00:02<00:04, 1705719.01it/s]Reading graph:  41%|████      | 4657652/11392560 [00:02<00:03, 1812270.47it/s]Reading graph:  43%|████▎     | 4867328/11392560 [00:02<00:03, 1890618.27it/s]Reading graph:  45%|████▍     | 5076582/11392560 [00:02<00:03, 1947629.35it/s]Reading graph:  46%|████▋     | 5290123/11392560 [00:02<00:03, 2001590.40it/s]Reading graph:  48%|████▊     | 5504968/11392560 [00:02<00:02, 2044317.17it/s]Reading graph:  50%|█████     | 5715063/11392560 [00:03<00:02, 2060925.44it/s]Reading graph:  52%|█████▏    | 5927206/11392560 [00:03<00:02, 2078782.43it/s]Reading graph:  54%|█████▍    | 6195191/11392560 [00:03<00:02, 2257203.89it/s]Reading graph:  57%|█████▋    | 6463820/11392560 [00:03<00:02, 2384955.78it/s]Reading graph:  59%|█████▉    | 6740251/11392560 [00:03<00:01, 2498144.62it/s]Reading graph:  62%|██████▏   | 7019510/11392560 [00:03<00:01, 2586122.38it/s]Reading graph:  64%|██████▍   | 7300961/11392560 [00:03<00:01, 2654429.96it/s]Reading graph:  67%|██████▋   | 7583406/11392560 [00:03<00:01, 2705305.23it/s]Reading graph:  69%|██████▉   | 7864024/11392560 [00:03<00:01, 2735493.21it/s]Reading graph:  71%|███████▏  | 8139482/11392560 [00:03<00:01, 2741175.75it/s]Reading graph:  74%|███████▍  | 8420585/11392560 [00:04<00:01, 2762082.02it/s]Reading graph:  76%|███████▋  | 8696879/11392560 [00:04<00:00, 2751961.71it/s]Reading graph:  79%|███████▉  | 8976875/11392560 [00:04<00:00, 2766289.93it/s]Reading graph:  81%|████████  | 9254793/11392560 [00:04<00:00, 2770118.08it/s]Reading graph:  84%|████████▎ | 9531842/11392560 [00:04<00:00, 2766855.08it/s]Reading graph:  86%|████████▌ | 9809261/11392560 [00:04<00:00, 2769014.66it/s]Reading graph:  89%|████████▊ | 10086182/11392560 [00:04<00:00, 2757386.20it/s]Reading graph:  91%|█████████ | 10365082/11392560 [00:04<00:00, 2766798.82it/s]Reading graph:  93%|█████████▎| 10642489/11392560 [00:04<00:00, 2768945.38it/s]Reading graph:  96%|█████████▌| 10921943/11392560 [00:04<00:00, 2776582.07it/s]Reading graph:  98%|█████████▊| 11203079/11392560 [00:05<00:00, 2786970.29it/s]                                                                               